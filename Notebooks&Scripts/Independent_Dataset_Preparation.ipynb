{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fed0840a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys, pysam, gzip, subprocess, shlex, time\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def give_inputs(cell_line):\n",
    "    \n",
    "    if cell_line == \"HEK293T\":  \n",
    "        samples = [[\"outTable_599710609\", \"outTable_905657585\", \"outTable_208420383\"],\n",
    "                   [\"outTable_572868058\", \"outTable_364841872\", \"outTable_814257267\"],\n",
    "                   [\"outTable_110067244\", \"outTable_597789462\", \"outTable_530905096\"]]\n",
    "\n",
    "        rmsk_file = \"rmsk_hg38.sorted.gtf.gz\"\n",
    "        refseq_file = \"hg38.110.ncbiRefSeq.sorted.gtf.gz\"\n",
    "\n",
    "    elif cell_line == \"HEK\":\n",
    "        samples = [[\"outTable_724242056\", \"outTable_816573740\"],\n",
    "                   [\"outTable_580067564\", \"outTable_718392497\"],\n",
    "                   [\"outTable_181728208\", \"outTable_854894021\"]]\n",
    "        rmsk_file = \"rmsk.sorted.gtf.gz\"\n",
    "        refseq_file = \"hg19.ncbiRefSeq.sorted.gtf.gz\"\n",
    "\n",
    "    else:\n",
    "        samples = [[\"outTable_192318299\", \"outTable_436061877\"],\n",
    "                   [\"outTable_535670354\", \"outTable_396704193\"],\n",
    "                   [\"outTable_773331943\", \"outTable_302610513\"]]\n",
    "        rmsk_file = \"rmsk.sorted.gtf.gz\"\n",
    "        refseq_file = \"hg19.ncbiRefSeq.sorted.gtf.gz\"\n",
    "    \n",
    "    return samples, rmsk_file, refseq_file\n",
    "\n",
    "def extraction(prefix):    \n",
    "    \n",
    "    cov_threshold = 50\n",
    "    AGfreq_threshold = 0.01\n",
    "    AG_min = 3\n",
    "    interval = 101\n",
    "\n",
    "    starttime = datetime.now()\n",
    "\n",
    "    editing = []\n",
    "    with gzip.open(prefix+\".gz\") as redi:\n",
    "        for c,l in enumerate(redi):\n",
    "            line = l.decode(\"utf-8\").rstrip().split(\"\\t\")\n",
    "            if line[0].find(\"chr\") != -1:\n",
    "                if line[4] != \"-\":\n",
    "                    if int(line[4]) >= cov_threshold:\n",
    "                        if line[2] == \"A\":\n",
    "                            if line[7] == \"AG\":    \n",
    "                                AG_rna = eval(line[6])[2]/sum(eval(line[6]))\n",
    "                                if AG_rna >= AGfreq_threshold:\n",
    "                                    if eval(line[6])[2] >= AG_min:\n",
    "                                        editing.append(line)\n",
    "                          \n",
    "            if c % 50000000 == 0:\n",
    "                print(f\"\\tSites evaluated: {c}\")\n",
    "    print(\"Total evaluated rows:\", c)\n",
    "    editing = pd.DataFrame(editing)\n",
    "    print(\"Total extracted Candidates Editing sites for current sample:\", editing.shape[0])\n",
    "    stoptime = datetime.now()\n",
    "    print(f\"[{datetime.now()}] Extraction of Editing Candidates finished for current sample. Elapsed time: {stoptime-starttime}.\")\n",
    "    columns = [\"Region\", \"Position\", \"Ref\", \"Strand\", \"Cov\", \"Qual\", \"Bases\", \"AllSubs\", \"Freq\", \"gCov\", \"gQual\", \"g[A,C,G,T]\", \"gAllSubs\", \"gFreq\"]\n",
    "    editing.columns = columns\n",
    "    print(f\"[{datetime.now()}] Starting extraction of intervals.\")\n",
    "    ohe = OneHotEncoder()\n",
    "    ohe.fit(np.array([\"A\", \"C\", \"G\", \"T\"]).reshape(-1, 1))\n",
    "\n",
    "    intervals = []\n",
    "    starttime_preds = datetime.now()\n",
    "    total_extracted = 0\n",
    "    features_extracted_filepath = prefix+ \"_feature_vectors.tsv\"\n",
    "    features_extracted = open(features_extracted_filepath, \"w\")\n",
    "\n",
    "    df = editing.query(\"Region != 'chrM'\")\n",
    "    print(f\"[{datetime.now()}] Loading reditable with tabix and pysam:\", prefix)\n",
    "    start_time = datetime.now()\n",
    "    srr = pysam.TabixFile(prefix+\".gz\")\n",
    "    with tqdm(total=df.shape[0], position=0, leave=True) as pbar:\n",
    "        for site in df.itertuples():\n",
    "            start = int(site.Position) - ((interval-1)/2)\n",
    "            stop = int(site.Position) + ((interval-1)/2)\n",
    "            AGrna = eval(site.Bases)[2]/sum(eval(site.Bases))\n",
    "            srr_interval = []\n",
    "            for s in srr.fetch(site.Region, start-1, stop):\n",
    "                srr_interval.append(s.split(\"\\t\"))\n",
    "            srr_interval = pd.DataFrame(srr_interval, columns=columns)\n",
    "            if srr_interval.shape[0] == interval and len(set(srr_interval[\"Strand\"])) == 1:\n",
    "                intervals.append([site.Region, site.Position, site.Ref, site.Strand, AGrna, site.Bases, start, stop, stop-start + 1, srr_interval.shape[0]])\n",
    "                total_extracted += 1\n",
    "                strand = site.Strand\n",
    "                seq = srr_interval.Ref.values.reshape(-1,1)\n",
    "                seq_ohe = ohe.transform(seq).toarray().T\n",
    "                vects_freqs = []\n",
    "                strands = []\n",
    "                vects = []\n",
    "                for vect in srr_interval[\"Bases\"]:\n",
    "                    vect = np.array(eval(vect))\n",
    "                    cov = sum(vect)\n",
    "                    vect_freqs = vect / cov\n",
    "                    vects_freqs.append(vect_freqs)\n",
    "                    vects.append(vect)\n",
    "                vects_freqs = np.array(vects_freqs).T\n",
    "                vects = np.array(vects).T\n",
    "                encoded_site = pd.concat([pd.DataFrame(seq_ohe), pd.DataFrame(vects_freqs)])\n",
    "                encoded_site.reset_index(drop=True, inplace=True)\n",
    "                if strand == 0: \n",
    "                    encoded_site = pd.DataFrame(np.flip(encoded_site.values, axis=1))\n",
    "                encoded_site.to_csv(features_extracted, mode=\"a\", sep=\"\\t\", header = None, index=None)\n",
    "            pbar.update(1)\n",
    "    intervals = pd.DataFrame(intervals)\n",
    "    print(f\"[{datetime.now()}] Total extracted Editing sites: {total_extracted}.\")\n",
    "    stop_time_global = datetime.now()\n",
    "    print(f\"[{datetime.now()}] Features Extraction Finished. Elapsed time {datetime.now()-starttime_preds}.\")\n",
    "    features_extracted.close()\n",
    "    \n",
    "    intervals.columns = [\"Region\", \"Position\", \"RefBase\", \"Strand\", \"FreqAGrna\", \"BasesCounts\", \"Start\", \"Stop\", \"Intlen\", \"TabixLen\"]\n",
    "    intervals.to_csv(prefix + \"_intervals.tsv\", sep=\"\\t\", index=None)\n",
    "    print(f\"[{datetime.now()}] Computation Finished. Total Elapsed time: {datetime.now()-starttime}\")\n",
    "    \n",
    "def candidates_bona_fide_extraction(names, path, cells):    \n",
    "\n",
    "    starttime = datetime.now()\n",
    "    sites = []\n",
    "    \n",
    "    \n",
    "    cov_threshold = 10\n",
    "    rna_cov_threshold = 50\n",
    "    AGfreq_threshold = 0.01\n",
    "    AG_min = 3\n",
    "    \n",
    "    wgs = pysam.TabixFile(f\"{path}/{cells}_WGS.gz\")\n",
    "    with gzip.open(f\"{path}/{names[0]}.gz\") as redi:\n",
    "        for c,l in enumerate(redi):\n",
    "            line = l.decode(\"utf-8\").rstrip().split(\"\\t\")\n",
    "            if line[0].find(\"chr\") != -1:\n",
    "                if line[0] != \"chrM\":\n",
    "                    if line[4] != \"-\":\n",
    "                        if int(line[4]) >= rna_cov_threshold:\n",
    "                            if line[2] == \"A\":\n",
    "                                if \"AG\" in line[7] == \"AG\":    \n",
    "                                    AG_rna = eval(line[6])[2]/sum(eval(line[6]))\n",
    "                                    if AG_rna >= AGfreq_threshold:\n",
    "                                        if eval(line[6])[2] >= AG_min:\n",
    "                                            region = line[0]\n",
    "                                            start = int(line[1])-1\n",
    "                                            stop = int(line[1])\n",
    "                                            for ROW_WGS in wgs.fetch(region, start, stop):\n",
    "                                                row_wgs = ROW_WGS.split(\"\\t\")\n",
    "                                                if row_wgs[9] !=  \"-\":\n",
    "                                                    if int(row_wgs[9])>=cov_threshold:\n",
    "                                                        if \"AG\" in row_wgs[12]:\n",
    "                                                            sites.append([line[0], line[1], 0])\n",
    "                                                        else:\n",
    "                                                            if row_wgs[12] == \"-\":\n",
    "                                                                if line[7] == \"AG\":\n",
    "                                                                    sites.append([line[0], line[1], 1]) \n",
    "\n",
    "            if c % 50000000 == 0:\n",
    "                print(f\"\\tSites evaluated: {c}\")\n",
    "                                                                            \n",
    "    with gzip.open(f\"{path}/{names[1]}.gz\") as redi:\n",
    "        for c,l in enumerate(redi):\n",
    "            line = l.decode(\"utf-8\").rstrip().split(\"\\t\")\n",
    "            if line[0].find(\"chr\") != -1:\n",
    "                if line[0] != \"chrM\":\n",
    "                    if line[4] != \"-\":\n",
    "                        if int(line[4]) >= rna_cov_threshold:\n",
    "                            if line[2] == \"A\":\n",
    "                                if \"AG\" in line[7] == \"AG\":    \n",
    "                                    AG_rna = eval(line[6])[2]/sum(eval(line[6]))\n",
    "                                    if AG_rna >= AGfreq_threshold:\n",
    "                                        if eval(line[6])[2] >= AG_min:\n",
    "                                            region = line[0]\n",
    "                                            start = int(line[1])-1\n",
    "                                            stop = int(line[1])\n",
    "                                            for ROW_WGS in wgs.fetch(region, start, stop):\n",
    "                                                row_wgs = ROW_WGS.split(\"\\t\")\n",
    "                                                if row_wgs[9] !=  \"-\":\n",
    "                                                    if int(row_wgs[9])>=cov_threshold:\n",
    "                                                        if \"AG\" in row_wgs[12]:\n",
    "                                                            sites.append([line[0], line[1], 0])\n",
    "                                                        else:\n",
    "                                                            if row_wgs[12] == \"-\":\n",
    "                                                                if line[7] == \"AG\":\n",
    "                                                                    sites.append([line[0], line[1], 1]) \n",
    "            if c % 50000000 == 0:\n",
    "                print(f\"\\tSites evaluated: {c}\")\n",
    "    print(\"Total evaluated rows:\", c)\n",
    "    sites = pd.DataFrame(sites)\n",
    "    columns = [\"Region\", \"Position\", \"Class\"]\n",
    "    sites.columns = columns\n",
    "    sites.drop_duplicates(subset=columns, keep=\"first\", inplace=True)\n",
    "    print(\"Total extracted candidates bona fide Sites for current samples:\", sites.shape[0])\n",
    "    stoptime = datetime.now()\n",
    "    print(f\"[{datetime.now()}] Extraction of candidates sites finished for current sample. Elapsed time: {stoptime-starttime}.\")\n",
    "    \n",
    "    sites.to_csv(f\"{path}/{names[0]}_{names[1]}_candidates_bona_fide_sites.tsv\", sep=\"\\t\", index=None)\n",
    "\n",
    "u_path = \"/lustrehome/pietrolucamazzacuva/filezilla-recas/scripts/utilities\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64b16dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cells = \"HEK\"\n",
    "\n",
    "tables, rmsk, refseq = give_inputs(cells)\n",
    "    \n",
    "path = \"/lustrehome/pietrolucamazzacuva/filezilla-recas/tissues/independent_datasets/{}\".format(cells)\n",
    "\n",
    "inputs = []\n",
    "for i in range(len(tables)):\n",
    "    for j in range(len(tables[i])):\n",
    "        inputs.append(os.path.join(path, f\"{tables[i][j]}\"))\n",
    "\n",
    "with Pool(9) as pool:\n",
    "    pool.map(extraction, inputs)\n",
    "    \n",
    "inputs = []\n",
    "if cells == \"HEK293T\":\n",
    "    for i in range(3):\n",
    "            inputs.append([[tables[i][0], tables[i][1]], path, cells])\n",
    "            inputs.append([[tables[i][2], tables[i][1]], path, cells])\n",
    "else:\n",
    "    for i in range(3):\n",
    "            inputs.append([[tables[i][0], tables[i][1]], path, cells])\n",
    "    \n",
    "with Pool(6) as pool:\n",
    "    pool.starmap(candidates_bona_fide_extraction, inputs) \n",
    "    \n",
    "for file_name in os.listdir(path):\n",
    "    if file_name.find(\"_candidates_bona_fide_sites.tsv\") !=-1:\n",
    "        df = pd.read_csv(os.path.join(path, file_name), sep=\"\\t\")\n",
    "        df.to_csv(os.path.join(path, file_name), sep=\"\\t\", index=None, header=False)\n",
    "        name = file_name.replace(\".tsv\", \"\")\n",
    "        cmd_sh = \"python3 {}/AnnotateTablePython3.py -a {}/{} -n rmsk -i {}/{}.tsv -o {}/{}.out.rmsk -u\".format(u_path, u_path, rmsk, path, name, path, name)\n",
    "        args = shlex.split(cmd_sh)\n",
    "        p = subprocess.Popen(args, env=dict(os.environ, PATH=\"/lustrehome/pietrolucamazzacuva/.conda/envs/tf/bin\"))\n",
    "    \n",
    "time.sleep(60)\n",
    "\n",
    "for name in os.listdir(path):\n",
    "    if name.find(\"rmsk\") != -1:\n",
    "        cmd_sh = \"python3 {}/AnnotateTablePython3.py -a {}/{} -i {}/{} -o {}/{}.refseq -u\".format(u_path, u_path, refseq, path, name, path, name)\n",
    "        args = shlex.split(cmd_sh)\n",
    "        p = subprocess.Popen(args, env=dict(os.environ, PATH=\"/lustrehome/pietrolucamazzacuva/.conda/envs/tf/bin\"))\n",
    "        \n",
    "time.sleep(60)\n",
    "\n",
    "cols = [\"Region\", \"Position\", \"Class\", \"RMSK-Rep\", \"RMSK-Reg\", \"RefSeq-Rep\", \"RefSeq-Reg\"]\n",
    "for file_name in os.listdir(path):\n",
    "    if file_name.find(\".refseq\") !=-1:\n",
    "        df = pd.read_table(os.path.join(path, file_name), header=None)\n",
    "        name = file_name.replace(\".out.rmsk.refseq\", \"_annoted.tsv\")\n",
    "        df.columns = cols\n",
    "        df.to_csv(os.path.join(path, name), sep=\"\\t\", index=None)\n",
    "        \n",
    "for file_name in os.listdir(path):\n",
    "    if file_name.find(\"candidates_bona_fide_sites_annoted.tsv\") !=-1:\n",
    "        bona_fide = pd.read_csv(os.path.join(path, file_name), sep=\"\\t\")\n",
    "        rep = bona_fide[(bona_fide.iloc[:, 3] != \"-\") & (bona_fide.iloc[:, 4] != \"-\")]\n",
    "        non_rep = bona_fide[(bona_fide.iloc[:, 3] == \"-\") & (bona_fide.iloc[:, 4] == \"-\")]\n",
    "\n",
    "        del bona_fide\n",
    "\n",
    "        non_rep_n = non_rep[non_rep.iloc[:, 2]==0]\n",
    "        non_rep_p = non_rep[non_rep.iloc[:, 2]==1]\n",
    "\n",
    "        del non_rep\n",
    "\n",
    "        non_rep_p = non_rep_p[(non_rep_p.iloc[:, 5] != \"-\") & (non_rep_p.iloc[:, 6] != \"-\")]\n",
    "        bona_fide = pd.concat([rep, non_rep_n, non_rep_p])\n",
    "\n",
    "        del rep, non_rep_n, non_rep_p\n",
    "        \n",
    "        name = file_name.replace(\"candidates_bona_fide_sites_annoted.tsv\", \"bona_fide_sites.tsv\")\n",
    "        bona_fide = bona_fide.sort_values([\"Region\", \"Position\"])\n",
    "        bona_fide.to_csv(os.path.join(path, name), sep=\"\\t\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b4bbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cells = \"a549\"\n",
    "\n",
    "tables, rmsk, refseq = give_inputs(cells)\n",
    "    \n",
    "path = \"/lustrehome/pietrolucamazzacuva/filezilla-recas/tissues/independent_datasets/{}\".format(cells)\n",
    "\n",
    "inputs = []\n",
    "for i in range(len(tables)):\n",
    "    for j in range(len(tables[i])):\n",
    "        inputs.append(os.path.join(path, f\"{tables[i][j]}\"))\n",
    "\n",
    "with Pool(9) as pool:\n",
    "    pool.map(extraction, inputs)\n",
    "    \n",
    "inputs = []\n",
    "if cells == \"HEK293T\":\n",
    "    for i in range(3):\n",
    "            inputs.append([[tables[i][0], tables[i][1]], path, cells])\n",
    "            inputs.append([[tables[i][2], tables[i][1]], path, cells])\n",
    "else:\n",
    "    for i in range(3):\n",
    "            inputs.append([[tables[i][0], tables[i][1]], path, cells])\n",
    "    \n",
    "with Pool(6) as pool:\n",
    "    pool.starmap(candidates_bona_fide_extraction, inputs) \n",
    "    \n",
    "for file_name in os.listdir(path):\n",
    "    if file_name.find(\"_candidates_bona_fide_sites.tsv\") !=-1:\n",
    "        df = pd.read_csv(os.path.join(path, file_name), sep=\"\\t\")\n",
    "        df.to_csv(os.path.join(path, file_name), sep=\"\\t\", index=None, header=False)\n",
    "        name = file_name.replace(\".tsv\", \"\")\n",
    "        cmd_sh = \"python3 {}/AnnotateTablePython3.py -a {}/{} -n rmsk -i {}/{}.tsv -o {}/{}.out.rmsk -u\".format(u_path, u_path, rmsk, path, name, path, name)\n",
    "        args = shlex.split(cmd_sh)\n",
    "        p = subprocess.Popen(args, env=dict(os.environ, PATH=\"/lustrehome/pietrolucamazzacuva/.conda/envs/tf/bin\"))\n",
    "    \n",
    "time.sleep(60)\n",
    "\n",
    "for name in os.listdir(path):\n",
    "    if name.find(\"rmsk\") != -1:\n",
    "        cmd_sh = \"python3 {}/AnnotateTablePython3.py -a {}/{} -i {}/{} -o {}/{}.refseq -u\".format(u_path, u_path, refseq, path, name, path, name)\n",
    "        args = shlex.split(cmd_sh)\n",
    "        p = subprocess.Popen(args, env=dict(os.environ, PATH=\"/lustrehome/pietrolucamazzacuva/.conda/envs/tf/bin\"))\n",
    "        \n",
    "time.sleep(60)\n",
    "\n",
    "cols = [\"Region\", \"Position\", \"Class\", \"RMSK-Rep\", \"RMSK-Reg\", \"RefSeq-Rep\", \"RefSeq-Reg\"]\n",
    "for file_name in os.listdir(path):\n",
    "    if file_name.find(\".refseq\") !=-1:\n",
    "        df = pd.read_table(os.path.join(path, file_name), header=None)\n",
    "        name = file_name.replace(\".out.rmsk.refseq\", \"_annoted.tsv\")\n",
    "        df.columns = cols\n",
    "        df.to_csv(os.path.join(path, name), sep=\"\\t\", index=None)\n",
    "        \n",
    "for file_name in os.listdir(path):\n",
    "    if file_name.find(\"candidates_bona_fide_sites_annoted.tsv\") !=-1:\n",
    "        bona_fide = pd.read_csv(os.path.join(path, file_name), sep=\"\\t\")\n",
    "        rep = bona_fide[(bona_fide.iloc[:, 3] != \"-\") & (bona_fide.iloc[:, 4] != \"-\")]\n",
    "        non_rep = bona_fide[(bona_fide.iloc[:, 3] == \"-\") & (bona_fide.iloc[:, 4] == \"-\")]\n",
    "\n",
    "        del bona_fide\n",
    "\n",
    "        non_rep_n = non_rep[non_rep.iloc[:, 2]==0]\n",
    "        non_rep_p = non_rep[non_rep.iloc[:, 2]==1]\n",
    "\n",
    "        del non_rep\n",
    "\n",
    "        non_rep_p = non_rep_p[(non_rep_p.iloc[:, 5] != \"-\") & (non_rep_p.iloc[:, 6] != \"-\")]\n",
    "        bona_fide = pd.concat([rep, non_rep_n, non_rep_p])\n",
    "\n",
    "        del rep, non_rep_n, non_rep_p\n",
    "        \n",
    "        name = file_name.replace(\"candidates_bona_fide_sites_annoted.tsv\", \"bona_fide_sites.tsv\")\n",
    "        bona_fide = bona_fide.sort_values([\"Region\", \"Position\"])\n",
    "        bona_fide.to_csv(os.path.join(path, name), sep=\"\\t\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "566dfb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSites evaluated: 0\tSites evaluated: 0\tSites evaluated: 0\tSites evaluated: 0\tSites evaluated: 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\tSites evaluated: 0\n",
      "\tSites evaluated: 0\n",
      "\tSites evaluated: 0\n",
      "\tSites evaluated: 0\n",
      "\tSites evaluated: 50000000\n",
      "\tSites evaluated: 50000000\n",
      "\tSites evaluated: 50000000\n",
      "\tSites evaluated: 50000000\n",
      "\tSites evaluated: 50000000\n",
      "\tSites evaluated: 50000000\n",
      "\tSites evaluated: 50000000\n",
      "\tSites evaluated: 50000000\n",
      "\tSites evaluated: 50000000\n",
      "\tSites evaluated: 100000000\n",
      "\tSites evaluated: 100000000\n",
      "\tSites evaluated: 100000000\n",
      "\tSites evaluated: 100000000\n",
      "\tSites evaluated: 100000000\n",
      "\tSites evaluated: 100000000\n",
      "\tSites evaluated: 100000000\n",
      "\tSites evaluated: 100000000\n",
      "\tSites evaluated: 100000000\n",
      "\tSites evaluated: 150000000\n",
      "\tSites evaluated: 150000000\n",
      "\tSites evaluated: 150000000\n",
      "\tSites evaluated: 150000000\n",
      "\tSites evaluated: 150000000\n",
      "\tSites evaluated: 150000000\n",
      "\tSites evaluated: 150000000\n",
      "\tSites evaluated: 150000000\n",
      "\tSites evaluated: 150000000\n",
      "Total evaluated rows: 194677901\n",
      "Total extracted Candidates Editing sites for current sample: 11705\n",
      "[2024-03-13 15:20:50.372448] Extraction of Editing Candidates finished for current sample. Elapsed time: 0:06:19.812204.\n",
      "[2024-03-13 15:20:50.378337] Starting extraction of intervals.\n",
      "[2024-03-13 15:20:50.397418] Loading reditable with tabix and pysam: /lustrehome/pietrolucamazzacuva/filezilla-recas/tissues/independent_datasets/HEK293T/outTable_905657585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 2195/11701 [00:08<00:27, 349.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSites evaluated: 200000000\n",
      "Total evaluated rows: 199831579\n",
      "Total extracted Candidates Editing sites for current sample: 24397\n",
      "[2024-03-13 15:20:58.671977] Extraction of Editing Candidates finished for current sample. Elapsed time: 0:06:28.110396.\n",
      "[2024-03-13 15:20:58.675555] Starting extraction of intervals.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|█▉        | 2231/11701 [00:08<00:27, 347.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-13 15:20:58.697273] Loading reditable with tabix and pysam: /lustrehome/pietrolucamazzacuva/filezilla-recas/tissues/independent_datasets/HEK293T/outTable_110067244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 41/24391 [00:00<01:00, 402.56it/s]s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSites evaluated: 200000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 120/24391 [00:00<01:12, 334.34it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSites evaluated: 200000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 295/24391 [00:01<02:00, 199.92it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSites evaluated: 200000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 830/24391 [00:02<01:01, 381.90it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSites evaluated: 200000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 869/24391 [00:03<01:18, 300.26it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSites evaluated: 200000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 3744/11701 [00:12<00:22, 349.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSites evaluated: 200000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 7061/11701 [00:22<00:13, 337.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total evaluated rows: 205675117\n",
      "Total extracted Candidates Editing sites for current sample: 11897\n",
      "[2024-03-13 15:21:13.160204] Extraction of Editing Candidates finished for current sample. Elapsed time: 0:06:42.599172.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|█▊        | 4476/24391 [00:14<01:02, 319.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-13 15:21:13.165135] Starting extraction of intervals.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 61%|██████    | 7106/11701 [00:22<00:12, 367.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-13 15:21:13.189624] Loading reditable with tabix and pysam: /lustrehome/pietrolucamazzacuva/filezilla-recas/tissues/independent_datasets/HEK293T/outTable_364841872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 5300/24391 [00:16<00:55, 342.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total evaluated rows: 208788107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██▏       | 5342/24391 [00:16<00:52, 363.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total extracted Candidates Editing sites for current sample: 11086\n",
      "[2024-03-13 15:21:15.618514] Extraction of Editing Candidates finished for current sample. Elapsed time: 0:06:45.056736.\n",
      "[2024-03-13 15:21:15.621970] Starting extraction of intervals.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|██████▊   | 7916/11701 [00:25<00:11, 330.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-13 15:21:15.638323] Loading reditable with tabix and pysam: /lustrehome/pietrolucamazzacuva/filezilla-recas/tissues/independent_datasets/HEK293T/outTable_597789462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11701/11701 [00:36<00:00, 324.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-13 15:21:26.599138] Total extracted Editing sites: 5094.\n",
      "[2024-03-13 15:21:26.603694] Features Extraction Finished. Elapsed time 0:00:36.220906.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 3497/11079 [00:10<00:26, 289.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-13 15:21:26.635652] Computation Finished. Total Elapsed time: 0:06:56.075431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11079/11079 [00:33<00:00, 331.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-13 15:21:49.201906] Total extracted Editing sites: 4770.\n",
      "[2024-03-13 15:21:49.206691] Features Extraction Finished. Elapsed time 0:00:33.580917.\n",
      "[2024-03-13 15:21:49.237716] Computation Finished. Total Elapsed time: 0:07:18.675954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11891/11891 [00:37<00:00, 317.86it/s]\n",
      " 69%|██████▊   | 16741/24391 [00:51<00:21, 359.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-13 15:21:50.689859] Total extracted Editing sites: 5077.\n",
      "[2024-03-13 15:21:50.693687] Features Extraction Finished. Elapsed time 0:00:37.523664.\n",
      "[2024-03-13 15:21:50.725102] Computation Finished. Total Elapsed time: 0:07:20.164089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24391/24391 [01:14<00:00, 327.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-13 15:22:13.313147] Total extracted Editing sites: 9960.\n",
      "[2024-03-13 15:22:13.318799] Features Extraction Finished. Elapsed time 0:01:14.639913.\n",
      "[2024-03-13 15:22:13.372075] Computation Finished. Total Elapsed time: 0:07:42.810522\n",
      "\tSites evaluated: 250000000\n",
      "\tSites evaluated: 250000000\n",
      "\tSites evaluated: 250000000\n",
      "\tSites evaluated: 250000000\n",
      "\tSites evaluated: 250000000\n",
      "Total evaluated rows: 266283468\n",
      "Total extracted Candidates Editing sites for current sample: 54617\n",
      "[2024-03-13 15:23:11.794610] Extraction of Editing Candidates finished for current sample. Elapsed time: 0:08:41.234186.\n",
      "[2024-03-13 15:23:11.798365] Starting extraction of intervals.\n",
      "[2024-03-13 15:23:11.840604] Loading reditable with tabix and pysam: /lustrehome/pietrolucamazzacuva/filezilla-recas/tissues/independent_datasets/HEK293T/outTable_208420383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 12113/54614 [00:39<02:12, 319.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total evaluated rows: 288775068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██▏       | 12147/54614 [00:39<02:11, 324.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total extracted Candidates Editing sites for current sample: 37723\n",
      "[2024-03-13 15:23:51.209800] Extraction of Editing Candidates finished for current sample. Elapsed time: 0:09:20.649137.\n",
      "[2024-03-13 15:23:51.214471] Starting extraction of intervals.\n",
      "[2024-03-13 15:23:51.246582] Loading reditable with tabix and pysam: /lustrehome/pietrolucamazzacuva/filezilla-recas/tissues/independent_datasets/HEK293T/outTable_572868058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 5650/37720 [00:19<02:17, 232.72it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total evaluated rows: 298335470\n",
      "Total extracted Candidates Editing sites for current sample: 66236\n",
      "[2024-03-13 15:24:10.803807] Extraction of Editing Candidates finished for current sample. Elapsed time: 0:09:40.242378.\n",
      "[2024-03-13 15:24:10.808023] Starting extraction of intervals.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 18212/54614 [00:58<02:02, 296.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-13 15:24:10.854412] Loading reditable with tabix and pysam: /lustrehome/pietrolucamazzacuva/filezilla-recas/tissues/independent_datasets/HEK293T/outTable_814257267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 5748/37720 [00:19<01:46, 300.91it/s]][W::hts_idx_load3] The index file is older than the data file: /lustrehome/pietrolucamazzacuva/filezilla-recas/tissues/independent_datasets/HEK293T/outTable_814257267.gz.tbi\n",
      "  0%|          | 312/66233 [00:00<03:12, 342.90it/s]]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total evaluated rows: 298198400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█▌        | 5997/37720 [00:20<01:53, 278.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total extracted Candidates Editing sites for current sample: 67410\n",
      "[2024-03-13 15:24:11.936566] Extraction of Editing Candidates finished for current sample. Elapsed time: 0:09:41.374607.\n",
      "[2024-03-13 15:24:11.941197] Starting extraction of intervals."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 347/66233 [00:01<03:28, 315.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|███▍      | 18540/54614 [01:00<02:10, 276.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-13 15:24:11.984454] Loading reditable with tabix and pysam: /lustrehome/pietrolucamazzacuva/filezilla-recas/tissues/independent_datasets/HEK293T/outTable_530905096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 380/66233 [00:01<04:51, 225.64it/s]s][W::hts_idx_load3] The index file is older than the data file: /lustrehome/pietrolucamazzacuva/filezilla-recas/tissues/independent_datasets/HEK293T/outTable_530905096.gz.tbi\n",
      "  3%|▎         | 2032/67408 [00:06<03:18, 329.63it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSites evaluated: 300000000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███▊      | 20591/54614 [01:06<01:37, 350.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37720/37720 [02:03<00:00, 305.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-13 15:25:55.021996] Total extracted Editing sites: 15241.\n",
      "[2024-03-13 15:25:55.027303] Features Extraction Finished. Elapsed time 0:02:03.809630.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 50818/54614 [02:43<00:12, 298.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-13 15:25:55.106218] Computation Finished. Total Elapsed time: 0:11:24.545583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 31052/67408 [01:44<01:48, 333.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSites evaluated: 350000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54614/54614 [02:55<00:00, 311.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-13 15:26:07.103990] Total extracted Editing sites: 22910.\n",
      "[2024-03-13 15:26:07.109484] Features Extraction Finished. Elapsed time 0:02:55.307312.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 34669/66233 [01:56<01:30, 348.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-13 15:26:07.220994] Computation Finished. Total Elapsed time: 0:11:36.660601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 42668/67408 [02:21<01:41, 243.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total evaluated rows: 369524874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████▍   | 42837/66233 [02:22<01:11, 325.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total extracted Candidates Editing sites for current sample: 54971\n",
      "[2024-03-13 15:26:33.754138] Extraction of Editing Candidates finished for current sample. Elapsed time: 0:12:03.194117.\n",
      "[2024-03-13 15:26:33.758422] Starting extraction of intervals.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|██████▎   | 42695/67408 [02:21<01:39, 249.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-13 15:26:33.799813] Loading reditable with tabix and pysam: /lustrehome/pietrolucamazzacuva/filezilla-recas/tissues/independent_datasets/HEK293T/outTable_599710609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 66233/66233 [03:39<00:00, 301.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-13 15:27:50.739831] Total extracted Editing sites: 27907.\n",
      "[2024-03-13 15:27:50.743865] Features Extraction Finished. Elapsed time 0:03:39.931942.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 66092/67408 [03:38<00:04, 278.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-13 15:27:50.871458] Computation Finished. Total Elapsed time: 0:13:20.310058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67408/67408 [03:42<00:00, 302.42it/s]\n",
      " 43%|████▎     | 23384/54968 [01:20<03:02, 173.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-13 15:27:54.957750] Total extracted Editing sites: 28354.\n",
      "[2024-03-13 15:27:54.962866] Features Extraction Finished. Elapsed time 0:03:43.018178.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|████▎     | 23422/54968 [01:21<02:19, 226.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-13 15:27:55.096165] Computation Finished. Total Elapsed time: 0:13:24.534234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54968/54968 [03:07<00:00, 292.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-13 15:29:41.756411] Total extracted Editing sites: 22284.\n",
      "[2024-03-13 15:29:41.760744] Features Extraction Finished. Elapsed time 0:03:07.998795.\n",
      "[2024-03-13 15:29:41.867681] Computation Finished. Total Elapsed time: 0:15:11.307689\n",
      "\tSites evaluated: 0\tSites evaluated: 0\n",
      "\tSites evaluated: 0\tSites evaluated: 0\tSites evaluated: 0\tSites evaluated: 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\tSites evaluated: 50000000\n",
      "\tSites evaluated: 50000000\n",
      "\tSites evaluated: 50000000\n",
      "\tSites evaluated: 50000000\n",
      "\tSites evaluated: 50000000\n",
      "\tSites evaluated: 50000000\n",
      "\tSites evaluated: 100000000\n",
      "\tSites evaluated: 100000000\n",
      "\tSites evaluated: 100000000\n",
      "\tSites evaluated: 100000000\n",
      "\tSites evaluated: 100000000\n",
      "\tSites evaluated: 100000000\n",
      "\tSites evaluated: 150000000\n",
      "\tSites evaluated: 150000000\n",
      "\tSites evaluated: 150000000\n",
      "\tSites evaluated: 150000000\n",
      "\tSites evaluated: 150000000\n",
      "\tSites evaluated: 150000000\n",
      "\tSites evaluated: 0\n",
      "\tSites evaluated: 200000000\n",
      "\tSites evaluated: 200000000\n",
      "\tSites evaluated: 200000000\n",
      "\tSites evaluated: 200000000\n",
      "\tSites evaluated: 200000000\n",
      "\tSites evaluated: 50000000\n",
      "\tSites evaluated: 250000000\n",
      "\tSites evaluated: 250000000\n",
      "\tSites evaluated: 250000000\n",
      "\tSites evaluated: 250000000\n",
      "\tSites evaluated: 250000000\n",
      "\tSites evaluated: 0\n",
      "\tSites evaluated: 0\n",
      "\tSites evaluated: 100000000\n",
      "\tSites evaluated: 300000000\n",
      "\tSites evaluated: 0\n",
      "\tSites evaluated: 0\n",
      "\tSites evaluated: 50000000\n",
      "\tSites evaluated: 50000000\n",
      "\tSites evaluated: 150000000\n",
      "\tSites evaluated: 350000000\n",
      "\tSites evaluated: 50000000\n",
      "\tSites evaluated: 50000000\n",
      "\tSites evaluated: 0\n",
      "\tSites evaluated: 100000000\n",
      "\tSites evaluated: 100000000\n",
      "\tSites evaluated: 200000000\n",
      "Total evaluated rows: 208788107\n",
      "Total extracted candidates bona fide Sites for current samples: 29917\n",
      "[2024-03-13 15:44:08.376066] Extraction of candidates sites finished for current sample. Elapsed time: 0:14:26.355156.\n",
      "\tSites evaluated: 100000000\n",
      "\tSites evaluated: 100000000\n",
      "\tSites evaluated: 50000000\n",
      "\tSites evaluated: 150000000\n",
      "\tSites evaluated: 150000000\n",
      "\tSites evaluated: 150000000\n",
      "\tSites evaluated: 150000000\n",
      "Total evaluated rows: 194677901\n",
      "Total extracted candidates bona fide Sites for current samples: 60259\n",
      "[2024-03-13 15:46:50.079349] Extraction of candidates sites finished for current sample. Elapsed time: 0:17:08.058850.\n",
      "\tSites evaluated: 100000000\n",
      "\tSites evaluated: 200000000\n",
      "Total evaluated rows: 205675117\n",
      "Total extracted candidates bona fide Sites for current samples: 43187\n",
      "[2024-03-13 15:47:18.890699] Extraction of candidates sites finished for current sample. Elapsed time: 0:17:36.870081.\n",
      "\tSites evaluated: 200000000\n",
      "\tSites evaluated: 200000000\n",
      "\tSites evaluated: 150000000\n",
      "Total evaluated rows: 205675117\n",
      "Total extracted candidates bona fide Sites for current samples: 71397\n",
      "[2024-03-13 15:48:39.029574] Extraction of candidates sites finished for current sample. Elapsed time: 0:18:57.008826.\n",
      "Total evaluated rows: 208788107\n",
      "Total extracted candidates bona fide Sites for current samples: 71857\n",
      "[2024-03-13 15:48:46.836178] Extraction of candidates sites finished for current sample. Elapsed time: 0:19:04.813975.\n",
      "Total evaluated rows: 194677901\n",
      "Total extracted candidates bona fide Sites for current samples: 60457\n",
      "[2024-03-13 15:50:11.545806] Extraction of candidates sites finished for current sample. Elapsed time: 0:20:29.525377.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pysam version used: 0.21.0\n",
      "Script time --> START: 13/03/2024 15:50:11\n",
      "Pysam version used: 0.21.0\n",
      "Script time --> START: 13/03/2024 15:50:12\n",
      "Pysam version used: 0.21.0\n",
      "Script time --> START: 13/03/2024 15:50:12\n",
      "Pysam version used: 0.21.0\n",
      "Script time --> START: 13/03/2024 15:50:12\n",
      "Pysam version used: 0.21.0\n",
      "Script time --> START: 13/03/2024 15:50:12\n",
      "Pysam version used: 0.21.0\n",
      "Script time --> START: 13/03/2024 15:50:12\n",
      "Table saved on /lustrehome/pietrolucamazzacuva/filezilla-recas/tissues/independent_datasets/HEK293T/outTable_110067244_outTable_597789462_candidates_bona_fide_sites.out.rmsk\n",
      "Script time --> END: 13/03/2024 15:50:17\n",
      "Table saved on /lustrehome/pietrolucamazzacuva/filezilla-recas/tissues/independent_datasets/HEK293T/outTable_572868058_outTable_364841872_candidates_bona_fide_sites.out.rmsk\n",
      "Script time --> END: 13/03/2024 15:50:19\n",
      "Table saved on /lustrehome/pietrolucamazzacuva/filezilla-recas/tissues/independent_datasets/HEK293T/outTable_599710609_outTable_905657585_candidates_bona_fide_sites.out.rmsk\n",
      "Script time --> END: 13/03/2024 15:50:21\n",
      "Table saved on /lustrehome/pietrolucamazzacuva/filezilla-recas/tissues/independent_datasets/HEK293T/outTable_208420383_outTable_905657585_candidates_bona_fide_sites.out.rmsk\n",
      "Script time --> END: 13/03/2024 15:50:22\n",
      "Table saved on /lustrehome/pietrolucamazzacuva/filezilla-recas/tissues/independent_datasets/HEK293T/outTable_530905096_outTable_597789462_candidates_bona_fide_sites.out.rmsk\n",
      "Script time --> END: 13/03/2024 15:50:24\n",
      "Table saved on /lustrehome/pietrolucamazzacuva/filezilla-recas/tissues/independent_datasets/HEK293T/outTable_814257267_outTable_364841872_candidates_bona_fide_sites.out.rmsk\n",
      "Script time --> END: 13/03/2024 15:50:24\n",
      "Pysam version used: 0.21.0\n",
      "Script time --> START: 13/03/2024 15:51:12\n",
      "Pysam version used: 0.21.0\n",
      "Script time --> START: 13/03/2024 15:51:12\n",
      "Pysam version used: 0.21.0\n",
      "Script time --> START: 13/03/2024 15:51:12\n",
      "Pysam version used: 0.21.0\n",
      "Script time --> START: 13/03/2024 15:51:12\n",
      "Pysam version used: 0.21.0\n",
      "Script time --> START: 13/03/2024 15:51:12\n",
      "Pysam version used: 0.21.0\n",
      "Script time --> START: 13/03/2024 15:51:12\n",
      "Table saved on /lustrehome/pietrolucamazzacuva/filezilla-recas/tissues/independent_datasets/HEK293T/outTable_110067244_outTable_597789462_candidates_bona_fide_sites.out.rmsk.refseq\n",
      "Script time --> END: 13/03/2024 15:51:24\n",
      "Table saved on /lustrehome/pietrolucamazzacuva/filezilla-recas/tissues/independent_datasets/HEK293T/outTable_572868058_outTable_364841872_candidates_bona_fide_sites.out.rmsk.refseq\n",
      "Script time --> END: 13/03/2024 15:51:27\n",
      "Table saved on /lustrehome/pietrolucamazzacuva/filezilla-recas/tissues/independent_datasets/HEK293T/outTable_599710609_outTable_905657585_candidates_bona_fide_sites.out.rmsk.refseq\n",
      "Script time --> END: 13/03/2024 15:51:31\n",
      "Table saved on /lustrehome/pietrolucamazzacuva/filezilla-recas/tissues/independent_datasets/HEK293T/outTable_208420383_outTable_905657585_candidates_bona_fide_sites.out.rmsk.refseq\n",
      "Script time --> END: 13/03/2024 15:51:32\n",
      "Table saved on /lustrehome/pietrolucamazzacuva/filezilla-recas/tissues/independent_datasets/HEK293T/outTable_814257267_outTable_364841872_candidates_bona_fide_sites.out.rmsk.refseq\n",
      "Script time --> END: 13/03/2024 15:51:35\n",
      "Table saved on /lustrehome/pietrolucamazzacuva/filezilla-recas/tissues/independent_datasets/HEK293T/outTable_530905096_outTable_597789462_candidates_bona_fide_sites.out.rmsk.refseq\n",
      "Script time --> END: 13/03/2024 15:51:35\n"
     ]
    }
   ],
   "source": [
    "cells = \"HEK293T\"\n",
    "\n",
    "tables, rmsk, refseq = give_inputs(cells)\n",
    "    \n",
    "path = \"/lustrehome/pietrolucamazzacuva/filezilla-recas/tissues/independent_datasets/{}\".format(cells)\n",
    "\n",
    "inputs = []\n",
    "for i in range(len(tables)):\n",
    "    for j in range(len(tables[i])):\n",
    "        inputs.append(os.path.join(path, f\"{tables[i][j]}\"))\n",
    "\n",
    "with Pool(9) as pool:\n",
    "    pool.map(extraction, inputs)\n",
    "    \n",
    "inputs = []\n",
    "if cells == \"HEK293T\":\n",
    "    for i in range(3):\n",
    "            inputs.append([[tables[i][0], tables[i][1]], path, cells])\n",
    "            inputs.append([[tables[i][2], tables[i][1]], path, cells])\n",
    "else:\n",
    "    for i in range(3):\n",
    "            inputs.append([[tables[i][0], tables[i][1]], path, cells])\n",
    "    \n",
    "with Pool(6) as pool:\n",
    "    pool.starmap(candidates_bona_fide_extraction, inputs) \n",
    "    \n",
    "for file_name in os.listdir(path):\n",
    "    if file_name.find(\"_candidates_bona_fide_sites.tsv\") !=-1:\n",
    "        df = pd.read_csv(os.path.join(path, file_name), sep=\"\\t\")\n",
    "        df.to_csv(os.path.join(path, file_name), sep=\"\\t\", index=None, header=False)\n",
    "        name = file_name.replace(\".tsv\", \"\")\n",
    "        cmd_sh = \"python3 {}/AnnotateTablePython3.py -a {}/{} -n rmsk -i {}/{}.tsv -o {}/{}.out.rmsk -u\".format(u_path, u_path, rmsk, path, name, path, name)\n",
    "        args = shlex.split(cmd_sh)\n",
    "        p = subprocess.Popen(args, env=dict(os.environ, PATH=\"/lustrehome/pietrolucamazzacuva/.conda/envs/tf/bin\"))\n",
    "    \n",
    "time.sleep(60)\n",
    "\n",
    "for name in os.listdir(path):\n",
    "    if name.find(\"rmsk\") != -1:\n",
    "        cmd_sh = \"python3 {}/AnnotateTablePython3.py -a {}/{} -i {}/{} -o {}/{}.refseq -u\".format(u_path, u_path, refseq, path, name, path, name)\n",
    "        args = shlex.split(cmd_sh)\n",
    "        p = subprocess.Popen(args, env=dict(os.environ, PATH=\"/lustrehome/pietrolucamazzacuva/.conda/envs/tf/bin\"))\n",
    "        \n",
    "time.sleep(60)\n",
    "\n",
    "cols = [\"Region\", \"Position\", \"Class\", \"RMSK-Rep\", \"RMSK-Reg\", \"RefSeq-Rep\", \"RefSeq-Reg\"]\n",
    "for file_name in os.listdir(path):\n",
    "    if file_name.find(\".refseq\") !=-1:\n",
    "        df = pd.read_table(os.path.join(path, file_name), header=None)\n",
    "        name = file_name.replace(\".out.rmsk.refseq\", \"_annoted.tsv\")\n",
    "        df.columns = cols\n",
    "        df.to_csv(os.path.join(path, name), sep=\"\\t\", index=None)\n",
    "        \n",
    "for file_name in os.listdir(path):\n",
    "    if file_name.find(\"candidates_bona_fide_sites_annoted.tsv\") !=-1:\n",
    "        bona_fide = pd.read_csv(os.path.join(path, file_name), sep=\"\\t\")\n",
    "        rep = bona_fide[(bona_fide.iloc[:, 3] != \"-\") & (bona_fide.iloc[:, 4] != \"-\")]\n",
    "        non_rep = bona_fide[(bona_fide.iloc[:, 3] == \"-\") & (bona_fide.iloc[:, 4] == \"-\")]\n",
    "\n",
    "        del bona_fide\n",
    "\n",
    "        non_rep_n = non_rep[non_rep.iloc[:, 2]==0]\n",
    "        non_rep_p = non_rep[non_rep.iloc[:, 2]==1]\n",
    "\n",
    "        del non_rep\n",
    "\n",
    "        non_rep_p = non_rep_p[(non_rep_p.iloc[:, 5] != \"-\") & (non_rep_p.iloc[:, 6] != \"-\")]\n",
    "        bona_fide = pd.concat([rep, non_rep_n, non_rep_p])\n",
    "\n",
    "        del rep, non_rep_n, non_rep_p\n",
    "        \n",
    "        name = file_name.replace(\"candidates_bona_fide_sites_annoted.tsv\", \"bona_fide_sites.tsv\")\n",
    "        bona_fide = bona_fide.sort_values([\"Region\", \"Position\"])\n",
    "        bona_fide.to_csv(os.path.join(path, name), sep=\"\\t\", index=None)\n",
    "        \n",
    "for i in range(3):\n",
    "    wt = pd.read_csv(os.path.join(path, f\"{tables[i][0]}_{tables[i][1]}_bona_fide_sites_first_filtering.tsv\"), sep=\"\\t\")\n",
    "    ove = pd.read_csv(os.path.join(path, f\"{tables[i][2]}_{tables[i][1]}_bona_fide_sites_first_filtering.tsv\"), sep=\"\\t\")\n",
    "    \n",
    "    wt_rep = wt[(wt.loc[:, \"RMSK-Rep\"]!=\"-\") & (wt.loc[:, \"RMSK-Reg\"]!=\"-\")]\n",
    "    wt_not_rep_p = wt[(wt.loc[:, \"Class\"]==1) & (wt.loc[:, \"RMSK-Rep\"]==\"-\") & (wt.loc[:, \"RMSK-Reg\"]==\"-\")]\n",
    "    wt_not_rep_n = wt[(wt.loc[:, \"Class\"]==0) & (wt.loc[:, \"RMSK-Rep\"]==\"-\") & (wt.loc[:, \"RMSK-Reg\"]==\"-\")]\n",
    "    \n",
    "    ove_rep = ove[(ove.loc[:, \"RMSK-Rep\"]!=\"-\") & (ove.loc[:, \"RMSK-Reg\"]!=\"-\")]\n",
    "    ove_not_rep_p = ove[(ove.loc[:, \"Class\"]==1) & (ove.loc[:, \"RMSK-Rep\"]==\"-\") & (ove.loc[:, \"RMSK-Reg\"]==\"-\")]\n",
    "    ove_not_rep_n = ove[(ove.loc[:, \"Class\"]==0) & (ove.loc[:, \"RMSK-Rep\"]==\"-\") & (ove.loc[:, \"RMSK-Reg\"]==\"-\")]\n",
    "    \n",
    "    not_rep_p = wt_not_rep_p.merge(ove_not_rep_p, how=\"inner\", on=cols)\n",
    "    \n",
    "    wt = pd.concat([wt_rep, not_rep_p, wt_not_rep_n], axis=0)\n",
    "    ove = pd.concat([ove_rep, not_rep_p, ove_not_rep_n], axis=0)\n",
    "    \n",
    "    wt.to_csv(os.path.join(path, f\"{tables[i][0]}_{tables[i][1]}_bona_fide_sites.tsv\"), sep=\"\\t\", index=None)\n",
    "    ove.to_csv(os.path.join(path, f\"{tables[i][2]}_{tables[i][1]}_bona_fide_sites.tsv\"), sep=\"\\t\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cc3e0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
