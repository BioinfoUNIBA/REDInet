{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c928ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N° candidates samples:\n",
      "7\n",
      "N° samples with WGS:\n",
      "7\n",
      "\tSites evaluated: 0\tSites evaluated: 0\tSites evaluated: 0\tSites evaluated: 0\tSites evaluated: 0\n",
      "\n",
      "\n",
      "\tSites evaluated: 0\n",
      "\tSites evaluated: 0\n",
      "\n",
      "\n",
      "\tSites evaluated: 50000000\n",
      "\tSites evaluated: 50000000\n",
      "\tSites evaluated: 50000000\n",
      "\tSites evaluated: 50000000\n",
      "\tSites evaluated: 50000000\n",
      "\tSites evaluated: 50000000\n",
      "\tSites evaluated: 50000000\n",
      "\tSites evaluated: 100000000\n",
      "\tSites evaluated: 100000000\n",
      "\tSites evaluated: 100000000\n",
      "\tSites evaluated: 100000000\n",
      "\tSites evaluated: 100000000\n",
      "\tSites evaluated: 100000000\n",
      "\tSites evaluated: 100000000\n",
      "\tSites evaluated: 150000000\n",
      "\tSites evaluated: 150000000\n",
      "\tSites evaluated: 150000000\n",
      "\tSites evaluated: 150000000\n",
      "\tSites evaluated: 150000000\n",
      "\tSites evaluated: 150000000\n",
      "\tSites evaluated: 150000000\n",
      "\tSites evaluated: 200000000\n",
      "\tSites evaluated: 200000000\n",
      "\tSites evaluated: 200000000\n",
      "\tSites evaluated: 200000000\n",
      "\tSites evaluated: 200000000\n",
      "\tSites evaluated: 200000000\n",
      "\tSites evaluated: 200000000\n",
      "\tSites evaluated: 250000000\n",
      "\tSites evaluated: 250000000\n",
      "\tSites evaluated: 250000000\n",
      "\tSites evaluated: 250000000\n",
      "\tSites evaluated: 300000000\n",
      "\tSites evaluated: 300000000\n",
      "N° unstranded samples with WGS:\n",
      "7\n",
      "N° viable samples:\n",
      "7\n",
      "N° samples in clusters:\n",
      "7\n",
      "N° clusters:\n",
      "1\n",
      "Time elapsed: 28498137.1247792 minutes\n"
     ]
    }
   ],
   "source": [
    "import os, sys, time, gzip \n",
    "import pandas as pd\n",
    "import os.path\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def strandness_evaluation(input_path, sample_id):\n",
    "    TypeOfRun = \"Unstranded\"\n",
    "    with gzip.open(os.path.join(input_path, sample_id, sample_id+\".gz\")) as redi:\n",
    "        for c,l in enumerate(redi):\n",
    "            line = l.decode(\"utf-8\").rstrip().split(\"\\t\")\n",
    "            if c % 50000000 == 0:\n",
    "                print(f\"\\tSites evaluated: {c}\", flush=True)\n",
    "            if str(line[3]) != \"2\":\n",
    "                TypeOfRun = \"Stranded\"\n",
    "                break\n",
    "    return TypeOfRun, sample_id\n",
    "\n",
    "execution_start = time.time()\n",
    "\n",
    "tissue = \"fallopian_tube\"\n",
    "\n",
    "tables_path = \"/lustre/biomed/epicardi/ncbi/dbGaP-6698/sra\"\n",
    "files_path = \"/lustrehome/pietrolucamazzacuva/filezilla-recas/tissues/{}/{}_dataset_files\".format(tissue, tissue)\n",
    "clusters_path =  \"/lustrehome/pietrolucamazzacuva/filezilla-recas/tissues/{}/{}_clusters\".format(tissue, tissue)\n",
    "\n",
    "samples_read = pd.read_csv(os.path.join(files_path, f\"{tissue}_REDIportal.csv\"))\n",
    "print(\"N° candidates samples:\", flush=True)\n",
    "print(f\"{len(samples_read)}\", flush=True)\n",
    "\n",
    "samples_read.dropna(inplace=True)\n",
    "samples_read.reset_index(drop=True, inplace=True)\n",
    "samples_read['Type of Run'] = \"-\"\n",
    "\n",
    "print(\"N° samples with WGS:\", flush=True)\n",
    "print(f\"{len(samples_read)}\", flush=True)\n",
    "\n",
    "inputs = []\n",
    "for sample in samples_read.loc[:, \"Sample\"].tolist():\n",
    "    inputs.append([tables_path, sample])\n",
    "\n",
    "with Pool(32) as pool:\n",
    "    for strandness, ID in pool.starmap(strandness_evaluation, inputs):\n",
    "        samples_read.at[samples_read.loc[samples_read['Sample'] == ID].index[0], 'Type of Run'] = strandness\n",
    "\n",
    "samples_read.to_csv (os.path.join(files_path, tissue+\"_strandness_filtered.csv\"), index=None)\n",
    "\n",
    "samples_read = samples_read[samples_read.iloc[:,14] == 'Unstranded']\n",
    "print(\"N° unstranded samples with WGS:\", flush=True)\n",
    "print(f\"{len(samples_read)}\", flush=True)\n",
    "\n",
    "\n",
    "for sample in samples_read.loc[:, \"Sample\"].tolist():\n",
    "\n",
    "    if os.path.exists(os.path.join(tables_path, sample, \"EditingGood/editing.gz\"))==False:\n",
    "        print(f\"Sample {sample} doesen't have the file editing.gz\", flush=True)\n",
    "        samples_read.drop(samples_read[samples_read.loc[:, \"Sample\"] ==  sample].index, axis=0, inplace=True)\n",
    "\n",
    "    elif os.path.exists(os.path.join(tables_path, sample, sample+\"_dna.txt.gz\"))==False:\n",
    "        print(f\"Sample {sample} doesen't have the file {sample}_dna.txt.gz\", flush=True)\n",
    "        samples_read.drop(samples_read[samples_read.loc[:, \"Sample\"] ==  sample].index, axis=0, inplace=True)\n",
    "\n",
    "samples_read.to_csv(os.path.join(files_path, tissue+\"_complete_samples.csv\"), index=None)\n",
    "\n",
    "print(f\"N° viable samples:\", flush=True)\n",
    "print(f\"{len(samples_read)}\", flush=True)\n",
    "\n",
    "n_clusters = len(samples_read) // 40\n",
    "clusters_dimensions = []\n",
    "if n_clusters > 0:\n",
    "    rest = len(samples_read) % 40\n",
    "    additional = rest // n_clusters\n",
    "    if additional > 0:\n",
    "        if additional <= 10:\n",
    "            rest_add = rest % n_clusters\n",
    "            if rest_add > n_clusters:\n",
    "                rest_add_add = rest_add // n_clusters\n",
    "                rest_add_rest = rest_add % n_clusters\n",
    "                lenght_cluster = 40 + additional + rest_add_add\n",
    "                for n in range(n_clusters):\n",
    "                    if rest_add > 0:\n",
    "                        clusters_dimensions.append(lenght_cluster + 1)\n",
    "                        rest_add_rest = rest_add_rest - 1\n",
    "                    else:\n",
    "                        clusters_dimensions.append(lenght_cluster)\n",
    "            else:\n",
    "                lenght_cluster = 40 + additional\n",
    "                for n in range(n_clusters):\n",
    "                    if rest_add > 0:\n",
    "                        clusters_dimensions.append(lenght_cluster + 1)\n",
    "                        rest_add = rest_add - 1\n",
    "                    else:\n",
    "                        clusters_dimensions.append(lenght_cluster)\n",
    "        else:\n",
    "            max_lenght_cluster = 40\n",
    "            min_lenght_cluster = rest\n",
    "            for n in range(n_clusters):\n",
    "                clusters_dimensions.append(max_lenght_cluster)\n",
    "            n_clusters = n_clusters +1\n",
    "            clusters_dimensions.append(min_lenght_cluster)\n",
    "    else:\n",
    "        lenght_cluster = 40\n",
    "        for n in range(n_clusters):\n",
    "            if rest > 0:\n",
    "                clusters_dimensions.append(lenght_cluster + 1)\n",
    "                rest = rest - 1\n",
    "            else:\n",
    "                clusters_dimensions.append(lenght_cluster)\n",
    "else:\n",
    "    clusters_dimensions.append(len(samples_read))\n",
    "    n_clusters = 1\n",
    "\n",
    "samples_read = samples_read.reset_index(drop=True)\n",
    "start = 0\n",
    "end = 0\n",
    "lenght = 0\n",
    "\n",
    "for cluster in range(n_clusters):\n",
    "    end = start + clusters_dimensions[cluster]\n",
    "    new_cluster = samples_read.iloc[start:end]\n",
    "    new_cluster.to_csv(os.path.join(clusters_path, f\"{tissue}_cluster_{cluster}.csv\"), index=None)\n",
    "    start = end\n",
    "    lenght = lenght + clusters_dimensions[cluster]\n",
    "\n",
    "print(f\"N° samples in clusters:\", flush=True)\n",
    "print(f\"{lenght}\", flush=True)\n",
    "print(\"N° clusters:\", flush=True)\n",
    "print(f\"{n_clusters}\", flush=True)\n",
    "\n",
    "execution_end = time.time()\n",
    "print(f\"Time elapsed: {(execution_end-execution_start)/60} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8fe92644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample SRR1101693 uploaded\n",
      "Sample SRR1074140 uploaded\n",
      "Sample SRR811938 uploaded\n",
      "Sample SRR1082520 uploaded\n",
      "Sample SRR1083776 uploaded\n",
      "Sample SRR1071359 uploaded\n",
      "Sample SRR1076584 uploaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustrehome/pietrolucamazzacuva/anaconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/pandas/core/indexing.py:1667: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = value\n",
      "/lustrehome/pietrolucamazzacuva/anaconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/pandas/core/indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n",
      "/lustrehome/pietrolucamazzacuva/anaconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/pandas/core/indexing.py:1667: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = value\n",
      "/lustrehome/pietrolucamazzacuva/anaconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/pandas/core/indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n",
      "/lustrehome/pietrolucamazzacuva/anaconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/pandas/core/indexing.py:1667: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = value\n",
      "/lustrehome/pietrolucamazzacuva/anaconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/pandas/core/indexing.py:1667: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = value\n",
      "/lustrehome/pietrolucamazzacuva/anaconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/pandas/core/indexing.py:1667: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = value\n",
      "/lustrehome/pietrolucamazzacuva/anaconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/pandas/core/indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n",
      "/lustrehome/pietrolucamazzacuva/anaconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/pandas/core/indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n",
      "/lustrehome/pietrolucamazzacuva/anaconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/pandas/core/indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n",
      "/lustrehome/pietrolucamazzacuva/anaconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/pandas/core/indexing.py:1667: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = value\n",
      "/lustrehome/pietrolucamazzacuva/anaconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/pandas/core/indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n",
      "/lustrehome/pietrolucamazzacuva/anaconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/pandas/core/indexing.py:1667: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = value\n",
      "/lustrehome/pietrolucamazzacuva/anaconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/pandas/core/indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n",
      "/lustrehome/pietrolucamazzacuva/anaconda3/envs/tensorflow-gpu/lib/python3.7/multiprocessing/pool.py:47: DtypeWarning: Columns (9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(itertools.starmap(args[0], args[1]))\n",
      "/lustrehome/pietrolucamazzacuva/anaconda3/envs/tensorflow-gpu/lib/python3.7/multiprocessing/pool.py:47: DtypeWarning: Columns (9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(itertools.starmap(args[0], args[1]))\n",
      "/lustrehome/pietrolucamazzacuva/anaconda3/envs/tensorflow-gpu/lib/python3.7/multiprocessing/pool.py:47: DtypeWarning: Columns (9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(itertools.starmap(args[0], args[1]))\n",
      "/lustrehome/pietrolucamazzacuva/anaconda3/envs/tensorflow-gpu/lib/python3.7/multiprocessing/pool.py:47: DtypeWarning: Columns (9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(itertools.starmap(args[0], args[1]))\n",
      "/lustrehome/pietrolucamazzacuva/anaconda3/envs/tensorflow-gpu/lib/python3.7/multiprocessing/pool.py:47: DtypeWarning: Columns (9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(itertools.starmap(args[0], args[1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N° positives canditates in sample SRR1101693: 32041\n",
      "N° positives canditates in sample SRR1082520: 49730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustrehome/pietrolucamazzacuva/anaconda3/envs/tensorflow-gpu/lib/python3.7/multiprocessing/pool.py:47: DtypeWarning: Columns (9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(itertools.starmap(args[0], args[1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N° positives canditates in sample SRR811938: 64853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustrehome/pietrolucamazzacuva/anaconda3/envs/tensorflow-gpu/lib/python3.7/multiprocessing/pool.py:47: DtypeWarning: Columns (9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(itertools.starmap(args[0], args[1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N° positives canditates in sample SRR1083776: 57361\n",
      "N° positives canditates in sample SRR1074140: 72567\n",
      "N° positives canditates in sample SRR1076584: 91555\n",
      "N° positives canditates in sample SRR1071359: 126386\n",
      "Sample SRR1071359 compleated\n",
      "Sample SRR1074140 compleated\n",
      "Sample SRR1076584 compleated\n",
      "Sample SRR1082520 compleated\n",
      "Sample SRR1083776 compleated\n",
      "Sample SRR1101693 compleated\n",
      "Sample SRR811938 compleated\n",
      "Time elapsed: 5.4 minutes\n"
     ]
    }
   ],
   "source": [
    "import os, gzip, sys, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def positives_finder(sample_id, input_path, output_path):\n",
    "\n",
    "    df = pd.read_table(os.path.join(input_path, sample_id, \"EditingGood/editing.gz\"), sep=\"\\t\", \n",
    "                       compression=\"gzip\", header=None, usecols=[0, 1, 7, 14, 15]) \n",
    "    print(f\"Sample {sample_id} uploaded\", flush=True)\n",
    "    df.columns = [\"Region\", \"Position\", \"Sub\", \"Rep\", \"Reg\"]\n",
    "    df = df[(df.loc[:, \"Sub\"]==\"AG\") | (df.loc[:, \"Sub\"]==\"TC\")]\n",
    "    alu = df[df.loc[:,\"Reg\"].str.find(\"Alu\")!=-1]\n",
    "    not_rep = df[(df.loc[:,\"Rep\"].str.find(\"-\")!=-1) & (df.loc[:, \"Reg\"].str.find(\"-\")!=-1)]\n",
    "    not_alu = df[(df.loc[:, \"Rep\"].str.find(\"-\")==-1) & (df.loc[:, \"Reg\"].str.find(\"-\")==-1) \n",
    "                 & (df.loc[:, \"Reg\"].str.find(\"Alu\")==-1)]\n",
    "\n",
    "    if alu.shape[0] > 0:\n",
    "        alu.reset_index(drop=True, inplace=True)\n",
    "        alu.loc[:, \"Ref_Base\"] = \"T\"\n",
    "        alu.loc[:, \"Annotation\"] = \"ALU\"\n",
    "        alu.loc[:, \"Class\"] = \"ALU_POSITIVE\"\n",
    "        index = alu[alu.loc[:, \"Sub\"] == \"AG\"].index.tolist()\n",
    "        alu.iloc[index, 5] = \"A\"\n",
    "\n",
    "    if not_alu.shape[0] > 0:\n",
    "        not_alu.reset_index(drop=True, inplace=True)   \n",
    "        not_alu.loc[:, \"Ref_Base\"] = \"T\"\n",
    "        not_alu.loc[:, \"Annotation\"] = \"NOT_ALU\"\n",
    "        not_alu.loc[:, \"Class\"] = \"NOT_ALU_POSITIVE\" \n",
    "        index = not_alu[not_alu.loc[:, \"Sub\"] == \"AG\"].index.tolist()\n",
    "        not_alu.iloc[index, 5] = \"A\"\n",
    "        alu = pd.concat([alu, not_alu], axis=0)\n",
    "\n",
    "    if not_rep.shape[0] > 0:\n",
    "        not_rep.reset_index(drop=True, inplace=True)   \n",
    "        not_rep.loc[:, \"Ref_Base\"] = \"T\"\n",
    "        not_rep.loc[:, \"Annotation\"] = \"NOT_REP\"\n",
    "        not_rep.loc[:, \"Class\"] = \"NOT_REP_POSITIVE\" \n",
    "        index = not_rep[not_rep.loc[:, \"Sub\"] == \"AG\"].index.tolist()\n",
    "        not_rep.iloc[index, 5] = \"A\"\n",
    "        alu = pd.concat([alu, not_rep], axis=0)\n",
    "       \n",
    "    del not_alu, not_rep\n",
    "\n",
    "    alu.drop([\"Sub\", \"Rep\", \"Reg\"], axis=1, inplace=True)\n",
    "\n",
    "    recoding = pd.read_csv(\"/lustrehome/pietrolucamazzacuva/filezilla-recas/tissues/additional_recoding_list.tsv\", sep=\"\\t\")\n",
    "   \n",
    "    df = pd.read_table(os.path.join(input_path, sample_id, sample_id+\"_dna.txt.gz\"), sep=\"\\t\", \n",
    "                       compression=\"gzip\", header=None, usecols=[0, 1, 7, 9, 12])\n",
    "    df.columns = [\"Region\", \"Position\", \"Sub_RNA\", \"G_Cov\", \"Sub_WGS\"]\n",
    "    \n",
    "    not_rep = df[((df[\"Sub_RNA\"]==\"AG\") | (df[\"Sub_RNA\"]==\"TC\")) & (df[\"Sub_WGS\"]==\"-\") & (df[\"G_Cov\"]!=\"-\")]\n",
    "    del df\n",
    "\n",
    "    not_rep = not_rep.merge(recoding, how=\"inner\", on=[\"Region\", \"Position\"])\n",
    "    if not_rep.shape[0] > 0:\n",
    "        not_rep.drop([\"G_Cov\", \"Sub_WGS\"], axis=1, inplace=True)\n",
    "        not_rep.reset_index(drop=True, inplace=True)   \n",
    "        not_rep.loc[:, \"Ref_Base\"] = \"T\"\n",
    "        not_rep.loc[:, \"Annotation\"] = \"NOT_ALU\"\n",
    "        not_rep.loc[:, \"Class\"] = \"NOT_REP_POSITIVE\" \n",
    "        index = not_rep[not_rep.loc[:, \"Sub_RNA\"] == \"AG\"].index.tolist()\n",
    "        not_rep.iloc[index, 3] = \"A\"\n",
    "        not_rep.drop(\"Sub_RNA\", axis=1, inplace=True)\n",
    "    alu = pd.concat([alu, not_rep], axis=0)\n",
    "\n",
    "    print(f\"N° positives canditates in sample {sample_id}: {alu.shape[0]}\", flush=True)\n",
    "    alu.to_csv(os.path.join(output_path, f\"{sample_id}_positives_candidates_list.tsv\"), sep=\"\\t\", index=None)\n",
    "    return \"Sample {} compleated\".format(sample_id)\n",
    "                \n",
    "tissue, cluster_number = \"fallopian_tube\", \"0\"\n",
    "\n",
    "tables_path = \"/lustre/biomed/epicardi/ncbi/dbGaP-6698/sra\"\n",
    "samples_path = \"/lustrehome/pietrolucamazzacuva/filezilla-recas/tissues/{}/{}_samples_lists\".format(tissue, tissue)\n",
    "\n",
    "samples = pd.read_csv(f\"/lustrehome/pietrolucamazzacuva/filezilla-recas/tissues/{tissue}/{tissue}_clusters/{tissue}_cluster_{cluster_number}.csv\", usecols=[\"Sample\"])\n",
    "samples = samples[\"Sample\"].tolist()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "with Pool(10) as pool:\n",
    "    for result in pool.starmap(positives_finder, [[sample, tables_path, samples_path] for sample in samples]):\n",
    "        print(result, flush=True)\n",
    "        \n",
    "end = time.time()\n",
    "print(f\"Time elapsed: {round((end-start)/60, 1)} minutes\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d442ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSites evaluated: 0\tSites evaluated: 0\tSites evaluated: 0\tSites evaluated: 0\tSites evaluated: 0\tSites evaluated: 0\n",
      "\tSites evaluated: 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\tSites evaluated: 50000000\n",
      "\tSites evaluated: 50000000\n",
      "\tSites evaluated: 50000000\n",
      "\tSites evaluated: 50000000\n",
      "\tSites evaluated: 50000000\n",
      "\tSites evaluated: 50000000\n",
      "\tSites evaluated: 50000000\n",
      "\tSites evaluated: 100000000\n",
      "\tSites evaluated: 100000000\n",
      "\tSites evaluated: 100000000\n",
      "\tSites evaluated: 100000000\n",
      "\tSites evaluated: 100000000\n",
      "\tSites evaluated: 100000000\n",
      "\tSites evaluated: 100000000\n",
      "\tSites evaluated: 150000000\n",
      "\tSites evaluated: 150000000\n",
      "\tSites evaluated: 150000000\n",
      "\tSites evaluated: 150000000\n",
      "\tSites evaluated: 150000000\n",
      "\tSites evaluated: 150000000\n",
      "\tSites evaluated: 150000000\n",
      "\tSites evaluated: 200000000\n",
      "\tSites evaluated: 200000000\n",
      "\tSites evaluated: 200000000\n",
      "\tSites evaluated: 200000000\n",
      "\tSites evaluated: 200000000\n",
      "\tSites evaluated: 200000000\n",
      "\tSites evaluated: 200000000\n",
      "N° SNPs candidates in sample SRR1082520: 940\n",
      "N° SNPs candidates in sample SRR1101693: 778\n",
      "N° SNPs candidates in sample SRR811938: 1429\n",
      "\tSites evaluated: 250000000\n",
      "\tSites evaluated: 250000000\n",
      "\tSites evaluated: 250000000\n",
      "\tSites evaluated: 250000000\n",
      "N° SNPs candidates in sample SRR1083776: 918\n",
      "N° SNPs candidates in sample SRR1074140: 940\n",
      "\tSites evaluated: 300000000\n",
      "\tSites evaluated: 300000000\n",
      "N° SNPs candidates in sample SRR1076584: 892\n",
      "N° SNPs candidates in sample SRR1071359: 1018\n",
      "Sample SRR1071359 compleated\n",
      "Sample SRR1074140 compleated\n",
      "Sample SRR1076584 compleated\n",
      "Sample SRR1082520 compleated\n",
      "Sample SRR1083776 compleated\n",
      "Sample SRR1101693 compleated\n",
      "Sample SRR811938 compleated\n"
     ]
    }
   ],
   "source": [
    "import os, gzip, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def finder(sample_id, input_path, output_path):\n",
    "\n",
    "    regions_positions = np.empty((0,5))\n",
    "    with gzip.open(os.path.join(input_path, sample_id, sample_id+\"_dna.txt.gz\")) as redi:\n",
    "        for c,l in enumerate(redi):\n",
    "            line = l.decode(\"utf-8\").rstrip().split(\"\\t\")\n",
    "            if line[2] == \"A\":\n",
    "                if line[4] != \"-\" and line[9] != \"-\":\n",
    "                    if int(line[4]) >= 50 and int(line[9]) >= 10:\n",
    "                        if \"AG\" in line[7]:\n",
    "                            if \"AG\" in line[12]:\n",
    "                                AG_rna = eval(line[6])[2]/sum(eval(line[6]))\n",
    "                                AG_wgs = eval(line[11])[2]/sum(eval(line[11]))\n",
    "                                if AG_wgs >= 0.4:\n",
    "                                    if (AG_rna / AG_wgs <= 1.05) and (AG_rna / AG_wgs >= 0.95):\n",
    "                                        regions_positions = np.append(regions_positions, [[line[0], int(line[1]), \"A\", \"NOT_ANNOTED\", \"SNP\"]], axis=0)\n",
    "            if c % 50000000 == 0:\n",
    "                print(f\"\\tSites evaluated: {c}\")\n",
    "\n",
    "    print(f\"N° SNPs candidates in sample {sample_id}: {regions_positions.shape[0]}\", flush=True)\n",
    "    coordinates_list = pd.DataFrame(data=regions_positions)\n",
    "    coordinates_list.columns = [\"Region\", \"Position\", \"Ref_Base\", \"Annotation\", \"Class\"]\n",
    "    coordinates_list.to_csv(os.path.join(output_path, f\"{sample_id}_snps_candidates_list.tsv\"), sep=\"\\t\", index=None)\n",
    "    del coordinates_list, regions_positions\n",
    "\n",
    "    return \"Sample {} compleated\".format(sample_id)\n",
    "\n",
    "tissue, cluster_number = \"fallopian_tube\", \"0\"\n",
    "\n",
    "tables_path = \"/lustre/biomed/epicardi/ncbi/dbGaP-6698/sra\"\n",
    "samples_path = \"/lustrehome/pietrolucamazzacuva/filezilla-recas/tissues/{}/{}_samples_lists\".format(tissue, tissue)\n",
    "\n",
    "samples = pd.read_csv(f\"/lustrehome/pietrolucamazzacuva/filezilla-recas/tissues/{tissue}/{tissue}_clusters/{tissue}_cluster_{cluster_number}.csv\", usecols=[\"Sample\"])\n",
    "samples = samples[\"Sample\"].tolist()\n",
    "\n",
    "\n",
    "with Pool(15) as pool:\n",
    "    for result in pool.starmap(finder, [[sample, tables_path, samples_path] for sample in samples]):\n",
    "        print(result, flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4da6e3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample SRR1082520 uploaded\n",
      "Sample SRR1101693 uploaded\n",
      "Sample SRR811938 uploaded\n",
      "Sample SRR1083776 uploaded\n",
      "Sample SRR1074140 uploaded\n",
      "N° zero editing candidates in sample SRR1082520: 381421\n",
      "N° zero editing candidates in sample SRR1101693: 173800\n",
      "Sample SRR1076584 uploaded\n",
      "N° zero editing candidates in sample SRR811938: 566117\n",
      "N° zero editing candidates in sample SRR1083776: 492454\n",
      "Sample SRR1071359 uploaded\n",
      "N° zero editing candidates in sample SRR1074140: 474455\n",
      "N° zero editing candidates in sample SRR1076584: 349353\n",
      "N° zero editing candidates in sample SRR1071359: 411256\n",
      "Sample SRR1071359 compleated\n",
      "Sample SRR1074140 compleated\n",
      "Sample SRR1076584 compleated\n",
      "Sample SRR1082520 compleated\n",
      "Sample SRR1083776 compleated\n",
      "Sample SRR1101693 compleated\n",
      "Sample SRR811938 compleated\n",
      "Time elapsed: 6.655000257492065 minutes\n"
     ]
    }
   ],
   "source": [
    "import os, sys, time\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def zeros_finder(sample_id, input_path, output_path):\n",
    "    zeros = pd.read_table(os.path.join(input_path, sample_id, sample_id+\"_dna.txt.gz\"), compression=\"gzip\", \n",
    "                          header=None, sep=\"\\t\", usecols=[0,1,2,4,7,9,12], low_memory=False)\n",
    "    print(f\"Sample {sample_id} uploaded\", flush=True)\n",
    "    zeros.columns = [f\"{i}\" for i in range(zeros.shape[1])]\n",
    "    zeros = zeros[(zeros[\"2\"]==\"A\") & (zeros[\"3\"]!=\"-\") & (zeros[\"5\"]!=\"-\") & (zeros[\"4\"]==\"-\") & (zeros[\"6\"]==\"-\")]\n",
    "    zeros.reset_index(drop=True, inplace=True)\n",
    "    zeros[\"3\"] = zeros[\"3\"].astype(\"int32\")\n",
    "    zeros[\"5\"] = zeros[\"5\"].astype(\"int32\")\n",
    "    zeros = zeros[(zeros[\"3\"]>=200) & (zeros[\"5\"]>=30)]\n",
    "    zeros = zeros.loc[:, [\"0\", \"1\"]]\n",
    "    zeros.columns = [\"Region\", \"Position\"]\n",
    "    zeros[\"Ref_Base\"] = \"A\"\n",
    "    zeros[\"Annotation\"] = \"NOT_ANNOTED\"\n",
    "    zeros[\"Class\"] = \"ZERO_NEGATIVE\"\n",
    "    \n",
    "    print(f\"N° zero editing candidates in sample {sample_id}: {zeros.shape[0]}\", flush=True)\n",
    "    zeros.to_csv(os.path.join(output_path, f\"{sample_id}_zeros_candidates_list.tsv\"), sep=\"\\t\", index=None)\n",
    "     \n",
    "    return \"Sample {} compleated\".format(sample_id)  \n",
    "\n",
    "tissue, cluster_number = \"fallopian_tube\", \"0\"\n",
    "\n",
    "tables_path = \"/lustre/biomed/epicardi/ncbi/dbGaP-6698/sra\"\n",
    "samples_path = \"/lustrehome/pietrolucamazzacuva/filezilla-recas/tissues/{}/{}_samples_lists\".format(tissue, tissue)\n",
    "\n",
    "samples = pd.read_csv(f\"/lustrehome/pietrolucamazzacuva/filezilla-recas/tissues/{tissue}/{tissue}_clusters/{tissue}_cluster_{cluster_number}.csv\", usecols=[\"Sample\"])\n",
    "samples = samples[\"Sample\"].tolist()\n",
    "\n",
    "start = time.time()\n",
    "with Pool(7) as pool:\n",
    "    for result in pool.starmap(zeros_finder, [[sample, tables_path, samples_path] for sample in samples]):\n",
    "        print(result, flush=True)\n",
    "end = time.time()\n",
    "print(f\"Time elapsed: {(end-start)/60} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67ff2c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analisys on :Fallopian Tube\n",
      "\tSample SRR1071359 uploaded\n",
      "\tSample SRR1074140 uploaded\n",
      "\tSample SRR1076584 uploaded\n",
      "\tSample SRR1082520 uploaded\n",
      "\tSample SRR1083776 uploaded\n",
      "\tSample SRR1101693 uploaded\n",
      "\tSample SRR811938 uploaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pysam version used: 0.21.0\n",
      "Script time --> START: 08/03/2024 15:52:36\n",
      "Table saved on /lustrehome/pietrolucamazzacuva/filezilla-recas/tissues/fallopian_tube/fallopian_tube_samples_lists/fallopian_tube_zeros_counts_filtered.out.rmsk\n",
      "Script time --> END: 08/03/2024 15:52:57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 1.399435245990753 minutes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os, subprocess, shlex, time\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def reader(path, sample):            \n",
    "    temp = pd.read_csv(os.path.join(path, f\"{str(sample)}_zeros_candidates_list.tsv\"), \n",
    "                                                sep=\"\\t\", usecols=[\"Region\", \"Position\"])\n",
    "    temp[\"Sample\"] = str(sample)\n",
    "    temp.Position = temp.Position.astype(\"int32\")\n",
    "    temp.Region = temp.Region.astype(\"str\")\n",
    "    return temp, \"\\tSample {} uploaded\".format(sample)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "tissue = \"fallopian_tube\"\n",
    "tissues_path = \"/lustrehome/pietrolucamazzacuva/filezilla-recas/tissues\"\n",
    "tissue_path = os.path.join(tissues_path, f\"{tissue}/{tissue}_samples_lists\")\n",
    "cluster_path = \"/lustrehome/pietrolucamazzacuva/filezilla-recas/tissues/{}/{}_clusters\".format(tissue, tissue)\n",
    "u_path = \"/lustrehome/pietrolucamazzacuva/filezilla-recas/scripts/utilities\"\n",
    "        \n",
    "samples_list = pd.DataFrame()\n",
    "for name in os.listdir(cluster_path):\n",
    "    temp = pd.read_csv(f\"{cluster_path}/{name}\")\n",
    "    samples_list = pd.concat([samples_list, temp], axis=0)\n",
    "samples_list.reset_index(drop=True, inplace=True)\n",
    "\n",
    "body_sites = samples_list.loc[:, \"Body Site\"].value_counts().to_dict()\n",
    "\n",
    "for body_site in list(body_sites.keys()):\n",
    "    print(f\"Analisys on :{body_site}\")\n",
    "\n",
    "    body_sites_samples = samples_list[samples_list[\"Body Site\"]==body_site]\n",
    "    body_sites_samples = body_sites_samples[\"Sample\"].tolist()\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    inputs = []\n",
    "    for sample in body_sites_samples:\n",
    "        inputs.append([tissue_path, sample])\n",
    "    with Pool(15) as pool:\n",
    "        for data, _ in pool.starmap(reader, inputs):\n",
    "            print(_, flush=True)\n",
    "            df = pd.concat([df, data], axis=0)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    dup = df.duplicated(subset = [\"Region\", \"Position\"], keep=False)\n",
    "    index = dup[dup.loc[:]==True].index.tolist()\n",
    "    df = df.iloc[index, :]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    contigs = df.iloc[:, 0].value_counts().index.tolist()\n",
    "    \n",
    "    final = pd.DataFrame()\n",
    "\n",
    "    for contig in contigs:\n",
    "        index = df[df.loc[:, \"Region\"]==contig].index.tolist()\n",
    "        counts = df.iloc[index, 1].value_counts()\n",
    "        temp = pd.DataFrame(data=[[contig for i in range(counts.shape[0])], counts.index.tolist(), counts.loc[:].tolist()]).T\n",
    "        temp.columns = [\"Region\", \"Position\", \"Raw Count\"]\n",
    "        temp.loc[:, \"Raw Count\"] = temp[\"Raw Count\"].astype(\"int32\")\n",
    "        temp = temp[temp.loc[:, \"Raw Count\"]>1]\n",
    "        temp.reset_index(drop=True, inplace=True)\n",
    "        temp.loc[:, \"Number of Samples (%)\"] = round((temp.loc[:, \"Raw Count\"]/body_sites[body_site])*100, 2)\n",
    "        temp = temp[temp.loc[:, \"Number of Samples (%)\"]>=70.0]\n",
    "        temp.drop([\"Raw Count\", \"Number of Samples (%)\"], axis=1, inplace=True)\n",
    "        final = pd.concat([final, temp], axis=0)\n",
    "    \n",
    "    file_name = \"{}_zeros_counts_filtered\".format(tissue)\n",
    "    final.to_csv(f\"{tissue_path}/{file_name}.tsv\", sep=\"\\t\", index=None)\n",
    "    cmd_sh = \"python3 {}/AnnotateTablePython3.py -a {}/rmsk.sorted.gtf.gz -n rmsk -i {}/{}.tsv -o {}/{}.out.rmsk -u\".format(u_path, u_path, tissue_path, file_name, tissue_path, file_name)\n",
    "    args = shlex.split(cmd_sh)\n",
    "    p = subprocess.Popen(args, env=dict(os.environ, PATH=\"/lustrehome/pietrolucamazzacuva/.conda/envs/tf/bin\"))\n",
    "\n",
    "time.sleep(60)\n",
    "\n",
    "zeros = pd.read_csv(os.path.join(tissue_path, f\"{tissue}_zeros_counts_filtered.out.rmsk\"), sep=\"\\t\")\n",
    "zeros=zeros[(zeros[\"rmsk_feat\"]==\"-\") & (zeros[\"rmsk_gid\"]==\"-\")]\n",
    "zeros = zeros.loc[:, [\"Region\", \"Position\"]]\n",
    "zeros.reset_index(drop=True, inplace=True)\n",
    "\n",
    "for name in os.listdir(tissue_path):\n",
    "    if name.find(\"zeros_candidates_list\") != -1:\n",
    "        temp = pd.read_csv(os.path.join(tissue_path, name), sep=\"\\t\")\n",
    "        temp = temp.merge(zeros, how=\"inner\", on=[\"Region\", \"Position\"])\n",
    "        temp.loc[:, \"Annotation\"] = \"NOT_REP_ZERO\"\n",
    "        file_name = name.replace(\"zeros\", \"zeros_filtered\")\n",
    "        temp.to_csv(os.path.join(tissue_path, file_name), sep=\"\\t\", index=None)\n",
    "        \n",
    "end = time.time()\n",
    "print(f\"Time elapsed: {(end-start)/60} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e2241e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-08 16:07:38.057434] Loading reditable with tabix and pysam:[2024-03-08 16:07:38.059824] Loading reditable with tabix and pysam:  /lustre/biomed/epicardi/ncbi/dbGaP-6698/sra/SRR811938/SRR811938_dna.txt.gz/lustre/biomed/epicardi/ncbi/dbGaP-6698/sra/SRR811938/SRR811938_dna.txt.gz\n",
      "\n",
      "[2024-03-08 16:07:38.075408] Loading reditable with tabix and pysam: /lustre/biomed/epicardi/ncbi/dbGaP-6698/sra/SRR811938/SRR811938_dna.txt.gz\n",
      "[2024-03-08 16:07:38.081661] Loading reditable with tabix and pysam: /lustre/biomed/epicardi/ncbi/dbGaP-6698/sra/SRR811938/SRR811938_dna.txt.gz\n",
      "[2024-03-08 16:07:38.096730] Loading reditable with tabix and pysam:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/99299 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " /lustre/biomed/epicardi/ncbi/dbGaP-6698/sra/SRR811938/SRR811938_dna.txt.gz\n",
      "[2024-03-08 16:07:38.107697] Loading reditable with tabix and pysam: /lustre/biomed/epicardi/ncbi/dbGaP-6698/sra/SRR811938/SRR811938_dna.txt.gz\n",
      "[2024-03-08 16:07:38.110845] Loading reditable with tabix and pysam: /lustre/biomed/epicardi/ncbi/dbGaP-6698/sra/SRR811938/SRR811938_dna.txt.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 66946/115469 [03:44<03:03, 263.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-08 16:11:22.746155] Computation for sample SRR1101693 finished. Elapsed time: 0:03:44.678951\n",
      "[2024-03-08 16:11:22.748209] Total extracted sites: 20300. in SampleSRR811938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 66608/66608 [03:44<00:00, 296.53it/s]\n",
      " 91%|█████████▏| 97435/106551 [05:27<00:22, 403.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-08 16:13:05.905478] Computation for sample SRR811938 finished. Elapsed time: 0:05:27.839343\n",
      "[2024-03-08 16:13:05.907397] Total extracted sites: 27284. in SampleSRR811938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99299/99299 [05:27<00:00, 302.92it/s]\n",
      " 82%|████████▏ | 100460/122524 [05:38<01:02, 353.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-08 16:13:16.565923] Computation for sample SRR1071359 finished. Elapsed time: 0:05:38.480109\n",
      "[2024-03-08 16:13:16.567929] Total extracted sites: 29282. in SampleSRR811938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101957/101957 [05:38<00:00, 301.25it/s]\n",
      " 87%|████████▋ | 106203/122524 [05:55<00:49, 327.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-08 16:13:34.095055] Computation for sample SRR1082520 finished. Elapsed time: 0:05:56.013626\n",
      "[2024-03-08 16:13:34.097117] Total extracted sites: 30710. in SampleSRR811938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106551/106551 [05:55<00:00, 299.35it/s]\n",
      " 94%|█████████▎| 114850/122524 [06:23<00:30, 247.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-08 16:14:01.283225] Computation for sample SRR1076584 finished. Elapsed time: 0:06:23.167744\n",
      "[2024-03-08 16:14:01.285328] Total extracted sites: 32994. in SampleSRR811938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115469/115469 [06:23<00:00, 301.39it/s]\n",
      " 99%|█████████▉| 121188/122524 [06:43<00:05, 236.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-08 16:14:21.589766] Computation for sample SRR1083776 finished. Elapsed time: 0:06:43.477010\n",
      "[2024-03-08 16:14:21.591639] Total extracted sites: 34921. in SampleSRR811938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 121432/121432 [06:43<00:00, 301.00it/s]\n",
      "100%|█████████▉| 122523/122524 [06:47<00:00, 335.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-08 16:14:25.976642] Computation for sample SRR1074140 finished. Elapsed time: 0:06:47.874738\n",
      "[2024-03-08 16:14:25.978509] Total extracted sites: 35336. in SampleSRR811938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 122524/122524 [06:47<00:00, 300.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-08 16:14:26.131490] Computation Finished. Time elapsed 0:00:06.806897 minutes\n",
      "Cluster 0 Sites Extracted:210827\n"
     ]
    }
   ],
   "source": [
    "import os, gzip, sys, time, pysam\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from multiprocessing import Pool\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def extractor(sample_id, input_path_1, input_path_2, output_path, ohe, interval, suffix):\n",
    "    \n",
    "    intervals = []\n",
    "    columns = [\"Region\", \"Position\", \"Ref\", \"Strand\", \"Cov\", \"Qual\", \"[A,C,G,T]\", \"AllSubs\", \"Freq\", \"gCov\", \"gQual\", \"g[A,C,G,T]\", \"gAllSubs\", \"gFreq\", \"type\", \"sample\"]\n",
    "    if suffix == \"zeros\":\n",
    "        df_sites_all = pd.read_csv(os.path.join(input_path_2, f\"{sample_id}_{suffix}_filtered_candidates_list.tsv\"), sep=\"\\t\")\n",
    "    else:\n",
    "        df_sites_all = pd.read_csv(os.path.join(input_path_2, f\"{sample_id}_{suffix}_candidates_list.tsv\"), sep=\"\\t\") \n",
    "    df_sites_all.columns = [\"region\", \"position\", \"ref_base\", \"annotation\", \"label\"]\n",
    "    df = df_sites_all.query(\"region != 'chrM'\")\n",
    "    srr_filepath = os.path.join(input_path_1, sample, sample+'_dna.txt.gz')\n",
    "    print(f\"[{datetime.now()}] Loading reditable with tabix and pysam:\", srr_filepath)\n",
    "    start_time = datetime.now()\n",
    "    srr = pysam.TabixFile(srr_filepath)\n",
    "\n",
    "    features_extracted_filepath = \"{}/{}_{}_features_{}_nucleotides.tsv\".format(output_path, sample_id, suffix, interval)\n",
    "    features_extracted = open(features_extracted_filepath, \"w\")\n",
    "\n",
    "    with tqdm(total=df.shape[0], position=0, leave=True) as pbar:\n",
    "        for site in df.itertuples():\n",
    "            start = site.position - ((interval-1)/2)\n",
    "            stop = site.position + ((interval-1)/2)\n",
    "            srr_interval = []\n",
    "            for s in srr.fetch(site.region, start-1, stop):\n",
    "                srr_interval.append(s.split(\"\\t\"))\n",
    "            srr_interval = pd.DataFrame(srr_interval, columns=columns[:-2])\n",
    "            if srr_interval.shape[0] == interval:\n",
    "                counts = srr_interval.AllSubs.value_counts()\n",
    "                if \"AG\" in counts.index:\n",
    "                    if counts.loc[\"AG\"] >= 2:\n",
    "                        intervals.append([site.region, site.position, site.ref_base, site.annotation, site.label, sample, start, stop, stop-start + 1, srr_interval.shape[0]])\n",
    "                        seq = srr_interval.Ref.values.reshape(-1,1)\n",
    "                        seq_ohe = ohe.transform(seq).toarray().T\n",
    "                        vects_freqs = []\n",
    "                        vects = []\n",
    "                        for vect in srr_interval[\"[A,C,G,T]\"]:\n",
    "                            vect = np.array(eval(vect))\n",
    "                            cov = sum(vect)\n",
    "                            vect_freqs = vect / cov\n",
    "                            vects_freqs.append(vect_freqs)\n",
    "                            vects.append(vect)\n",
    "                        vects_freqs = np.array(vects_freqs).T\n",
    "                        vects = np.array(vects).T\n",
    "                        site = pd.concat([pd.DataFrame(seq_ohe), pd.DataFrame(vects_freqs)])\n",
    "                        site.to_csv(features_extracted, mode=\"a\", sep=\"\\t\", header = None, index=None)\n",
    "            pbar.update(1)\n",
    "   \n",
    "        intervals = pd.DataFrame(intervals, columns=[\"Region\", \"Position\", \"Ref_base\", \"Annotation\",\"Type\", \"SRR\", \"Start\", \"Stop\", \"DeltaStartStop\", \"TabixIntervalLen\"])\n",
    "        intervals.to_csv(os.path.join(output_path, f\"{sample_id}_{suffix}_feature_vectors_metadata.tsv\"), sep=\"\\t\", index=None)\n",
    "        lenght = len(intervals)\n",
    "        print(f\"[{datetime.now()}] Computation for sample {sample_id} finished. Elapsed time: {datetime.now()-start_time}\")\n",
    "        print(f\"[{datetime.now()}] Total extracted sites: {lenght}. in Sample{sample}\")\n",
    "\n",
    "        return lenght\n",
    "\n",
    "tissue, cluster_number, nucleotides_number, suffix = \"fallopian_tube\", 0, 101, \"zeros\"\n",
    "\n",
    "tables_path = \"/lustre/biomed/epicardi/ncbi/dbGaP-6698/sra\"\n",
    "samples_path = \"/lustrehome/pietrolucamazzacuva/filezilla-recas/tissues/{}/{}_samples_lists\".format(tissue, tissue)\n",
    "features_path = \"/lustrehome/pietrolucamazzacuva/filezilla-recas/tissues/{}/{}_datasets\".format(tissue, tissue)\n",
    "if not os.path.isdir(features_path):\n",
    "    os.mkdir(features_path)\n",
    "\n",
    "samples = pd.read_csv(f\"/lustrehome/pietrolucamazzacuva/filezilla-recas/tissues/{tissue}/{tissue}_clusters/{tissue}_cluster_{cluster_number}.csv\", usecols=[\"Sample\"])\n",
    "samples = samples[\"Sample\"].tolist()\n",
    "onehot = OneHotEncoder()\n",
    "onehot.fit(np.array([\"A\", \"C\", \"G\", \"T\"]).reshape(-1, 1))\n",
    "\n",
    "total_sites = 0\n",
    "start_time_global = datetime.now()\n",
    "\n",
    "with Pool(15) as pool:\n",
    "    for sites in pool.starmap(extractor, [[sample, tables_path, samples_path, features_path, onehot, nucleotides_number, suffix] for sample in samples]):\n",
    "        total_sites += sites\n",
    "        \n",
    "stop_time_global = datetime.now()\n",
    "print(f\"[{datetime.now()}] Computation Finished. Time elapsed {(stop_time_global-start_time_global)/60} minutes\")\n",
    "print(f\"Cluster {cluster_number} Sites Extracted:{total_sites}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3eac663b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-08 18:29:19.715073] Loading reditable with tabix and pysam: /lustre/biomed/epicardi/ncbi/dbGaP-6698/sra/SRR811938/SRR811938_dna.txt.gz\n",
      "[2024-03-08 18:29:19.737275] Loading reditable with tabix and pysam: /lustre/biomed/epicardi/ncbi/dbGaP-6698/sra/SRR811938/SRR811938_dna.txt.gz\n",
      "[2024-03-08 18:29:19.747674] Loading reditable with tabix and pysam: /lustre/biomed/epicardi/ncbi/dbGaP-6698/sra/SRR811938/SRR811938_dna.txt.gz\n",
      "[2024-03-08 18:29:19.753690] Loading reditable with tabix and pysam: /lustre/biomed/epicardi/ncbi/dbGaP-6698/sra/SRR811938/SRR811938_dna.txt.gz\n",
      "[2024-03-08 18:29:19.767923] Loading reditable with tabix and pysam: /lustre/biomed/epicardi/ncbi/dbGaP-6698/sra/SRR811938/SRR811938_dna.txt.gz\n",
      "[2024-03-08 18:29:19.776849] Loading reditable with tabix and pysam: /lustre/biomed/epicardi/ncbi/dbGaP-6698/sra/SRR811938/SRR811938_dna.txt.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/49730 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-08 18:29:19.830418] Loading reditable with tabix and pysam:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/32041 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " /lustre/biomed/epicardi/ncbi/dbGaP-6698/sra/SRR811938/SRR811938_dna.txt.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 35886/49730 [01:47<00:48, 286.47it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-08 18:31:07.268368] Computation for sample SRR1101693 finished. Elapsed time: 0:01:47.545160\n",
      "[2024-03-08 18:31:07.270876] Total extracted sites: 5608. in SampleSRR811938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32041/32041 [01:47<00:00, 298.22it/s]\n",
      " 73%|███████▎  | 47442/64853 [02:28<01:02, 279.42it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-08 18:31:48.720470] Computation for sample SRR1082520 finished. Elapsed time: 0:02:28.960054\n",
      "[2024-03-08 18:31:48.722894] Total extracted sites: 7691. in SampleSRR811938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49730/49730 [02:28<00:00, 334.00it/s]\n",
      " 84%|████████▍ | 54402/64853 [02:51<00:29, 353.41it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-08 18:32:11.188118] Computation for sample SRR1083776 finished. Elapsed time: 0:02:51.444311\n",
      "[2024-03-08 18:32:11.190653] Total extracted sites: 8927. in SampleSRR811938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57361/57361 [02:51<00:00, 334.72it/s]\n",
      " 64%|██████▍   | 80882/126386 [03:22<02:32, 298.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-08 18:32:42.221021] Computation for sample SRR811938 finished. Elapsed time: 0:03:22.466757\n",
      "[2024-03-08 18:32:42.222922] Total extracted sites: 13381. in SampleSRR811938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64853/64853 [03:22<00:00, 320.41it/s]\n",
      " 68%|██████▊   | 85660/126386 [03:34<01:16, 529.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-08 18:32:54.098540] Computation for sample SRR1074140 finished. Elapsed time: 0:03:34.315401\n",
      "[2024-03-08 18:32:54.100224] Total extracted sites: 10532. in SampleSRR811938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72567/72567 [03:34<00:00, 338.66it/s]\n",
      " 77%|███████▋  | 97173/126386 [04:02<00:45, 640.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-08 18:33:22.450219] Computation for sample SRR1076584 finished. Elapsed time: 0:04:02.675756\n",
      "[2024-03-08 18:33:22.452620] Total extracted sites: 10394. in SampleSRR811938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91555/91555 [04:02<00:00, 377.32it/s]\n",
      "100%|█████████▉| 126357/126386 [05:09<00:00, 388.72it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-08 18:34:29.757284] Computation for sample SRR1071359 finished. Elapsed time: 0:05:09.921659\n",
      "[2024-03-08 18:34:29.759574] Total extracted sites: 11419. in SampleSRR811938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126386/126386 [05:09<00:00, 407.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-08 18:34:29.918898] Computation Finished. Time elapsed 0:00:05.174182 minutes\n",
      "Cluster 0 Sites Extracted:67952\n"
     ]
    }
   ],
   "source": [
    "import os, gzip, sys, time, pysam\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from multiprocessing import Pool\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def extractor(sample_id, input_path_1, input_path_2, output_path, ohe, interval, suffix):\n",
    "    \n",
    "    intervals = []\n",
    "    columns = [\"Region\", \"Position\", \"Ref\", \"Strand\", \"Cov\", \"Qual\", \"[A,C,G,T]\", \"AllSubs\", \"Freq\", \"gCov\", \"gQual\", \"g[A,C,G,T]\", \"gAllSubs\", \"gFreq\", \"type\", \"sample\"]\n",
    "    if suffix == \"zeros\":\n",
    "        df_sites_all = pd.read_csv(os.path.join(input_path_2, f\"{sample_id}_{suffix}_filtered_candidates_list.tsv\"), sep=\"\\t\")\n",
    "    else:\n",
    "        df_sites_all = pd.read_csv(os.path.join(input_path_2, f\"{sample_id}_{suffix}_candidates_list.tsv\"), sep=\"\\t\") \n",
    "    df_sites_all.columns = [\"region\", \"position\", \"ref_base\", \"annotation\", \"label\"]\n",
    "    df = df_sites_all.query(\"region != 'chrM'\")\n",
    "    srr_filepath = os.path.join(input_path_1, sample, sample+'_dna.txt.gz')\n",
    "    print(f\"[{datetime.now()}] Loading reditable with tabix and pysam:\", srr_filepath)\n",
    "    start_time = datetime.now()\n",
    "    srr = pysam.TabixFile(srr_filepath)\n",
    "\n",
    "    features_extracted_filepath = \"{}/{}_{}_features_{}_nucleotides.tsv\".format(output_path, sample_id, suffix, interval)\n",
    "    features_extracted = open(features_extracted_filepath, \"w\")\n",
    "\n",
    "    with tqdm(total=df.shape[0], position=0, leave=True) as pbar:\n",
    "        for site in df.itertuples():\n",
    "            start = site.position - ((interval-1)/2)\n",
    "            stop = site.position + ((interval-1)/2)\n",
    "            srr_interval = []\n",
    "            for s in srr.fetch(site.region, start-1, stop):\n",
    "                srr_interval.append(s.split(\"\\t\"))\n",
    "            srr_interval = pd.DataFrame(srr_interval, columns=columns[:-2])\n",
    "            if srr_interval.shape[0] == interval:\n",
    "                counts = srr_interval.AllSubs.value_counts()\n",
    "                if \"AG\" in counts.index:\n",
    "                    if counts.loc[\"AG\"] >= 2:\n",
    "                        intervals.append([site.region, site.position, site.ref_base, site.annotation, site.label, sample, start, stop, stop-start + 1, srr_interval.shape[0]])\n",
    "                        seq = srr_interval.Ref.values.reshape(-1,1)\n",
    "                        seq_ohe = ohe.transform(seq).toarray().T\n",
    "                        vects_freqs = []\n",
    "                        vects = []\n",
    "                        for vect in srr_interval[\"[A,C,G,T]\"]:\n",
    "                            vect = np.array(eval(vect))\n",
    "                            cov = sum(vect)\n",
    "                            vect_freqs = vect / cov\n",
    "                            vects_freqs.append(vect_freqs)\n",
    "                            vects.append(vect)\n",
    "                        vects_freqs = np.array(vects_freqs).T\n",
    "                        vects = np.array(vects).T\n",
    "                        site = pd.concat([pd.DataFrame(seq_ohe), pd.DataFrame(vects_freqs)])\n",
    "                        site.to_csv(features_extracted, mode=\"a\", sep=\"\\t\", header = None, index=None)\n",
    "            pbar.update(1)\n",
    "   \n",
    "        intervals = pd.DataFrame(intervals, columns=[\"Region\", \"Position\", \"Ref_base\", \"Annotation\",\"Type\", \"SRR\", \"Start\", \"Stop\", \"DeltaStartStop\", \"TabixIntervalLen\"])\n",
    "        intervals.to_csv(os.path.join(output_path, f\"{sample_id}_{suffix}_feature_vectors_metadata.tsv\"), sep=\"\\t\", index=None)\n",
    "        lenght = len(intervals)\n",
    "        print(f\"[{datetime.now()}] Computation for sample {sample_id} finished. Elapsed time: {datetime.now()-start_time}\")\n",
    "        print(f\"[{datetime.now()}] Total extracted sites: {lenght}. in Sample{sample}\")\n",
    "\n",
    "        return lenght\n",
    "\n",
    "tissue, cluster_number, nucleotides_number, suffix = \"fallopian_tube\", 0, 101, \"positives\"\n",
    "\n",
    "tables_path = \"/lustre/biomed/epicardi/ncbi/dbGaP-6698/sra\"\n",
    "samples_path = \"/lustrehome/pietrolucamazzacuva/filezilla-recas/tissues/{}/{}_samples_lists\".format(tissue, tissue)\n",
    "features_path = \"/lustrehome/pietrolucamazzacuva/filezilla-recas/tissues/{}/{}_datasets\".format(tissue, tissue)\n",
    "if not os.path.isdir(features_path):\n",
    "    os.mkdir(features_path)\n",
    "\n",
    "samples = pd.read_csv(f\"/lustrehome/pietrolucamazzacuva/filezilla-recas/tissues/{tissue}/{tissue}_clusters/{tissue}_cluster_{cluster_number}.csv\", usecols=[\"Sample\"])\n",
    "samples = samples[\"Sample\"].tolist()\n",
    "onehot = OneHotEncoder()\n",
    "onehot.fit(np.array([\"A\", \"C\", \"G\", \"T\"]).reshape(-1, 1))\n",
    "\n",
    "total_sites = 0\n",
    "start_time_global = datetime.now()\n",
    "\n",
    "with Pool(15) as pool:\n",
    "    for sites in pool.starmap(extractor, [[sample, tables_path, samples_path, features_path, onehot, nucleotides_number, suffix] for sample in samples]):\n",
    "        total_sites += sites\n",
    "        \n",
    "stop_time_global = datetime.now()\n",
    "print(f\"[{datetime.now()}] Computation Finished. Time elapsed {(stop_time_global-start_time_global)/60} minutes\")\n",
    "print(f\"Cluster {cluster_number} Sites Extracted:{total_sites}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1fb5f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-08 18:34:42.169800] Loading reditable with tabix and pysam: /lustre/biomed/epicardi/ncbi/dbGaP-6698/sra/SRR811938/SRR811938_dna.txt.gz[2024-03-08 18:34:42.174060] Loading reditable with tabix and pysam:\n",
      "[2024-03-08 18:34:42.175892] Loading reditable with tabix and pysam: [2024-03-08 18:34:42.175771] Loading reditable with tabix and pysam: /lustre/biomed/epicardi/ncbi/dbGaP-6698/sra/SRR811938/SRR811938_dna.txt.gz\n",
      " /lustre/biomed/epicardi/ncbi/dbGaP-6698/sra/SRR811938/SRR811938_dna.txt.gz/lustre/biomed/epicardi/ncbi/dbGaP-6698/sra/SRR811938/SRR811938_dna.txt.gz\n",
      "[2024-03-08 18:34:42.181555] Loading reditable with tabix and pysam:\n",
      " /lustre/biomed/epicardi/ncbi/dbGaP-6698/sra/SRR811938/SRR811938_dna.txt.gz\n",
      "[2024-03-08 18:34:42.189850] Loading reditable with tabix and pysam: /lustre/biomed/epicardi/ncbi/dbGaP-6698/sra/SRR811938/SRR811938_dna.txt.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/778 [00:00<?, ?it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-08 18:34:42.277418] Loading reditable with tabix and pysam: /lustre/biomed/epicardi/ncbi/dbGaP-6698/sra/SRR811938/SRR811938_dna.txt.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 805/918 [00:02<00:00, 312.88it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-08 18:34:44.886599] Computation for sample SRR1101693 finished. Elapsed time: 0:00:02.690328\n",
      "[2024-03-08 18:34:44.888329] Total extracted sites: 190. in SampleSRR811938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 778/778 [00:02<00:00, 295.36it/s]\n",
      " 97%|█████████▋| 915/940 [00:02<00:00, 307.93it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-08 18:34:45.137566] Computation for sample SRR1076584 finished. Elapsed time: 0:00:02.853354\n",
      "[2024-03-08 18:34:45.139343] Total extracted sites: 183. in SampleSRR811938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 892/892 [00:02<00:00, 318.12it/s]\n",
      " 99%|█████████▊| 906/918 [00:02<00:00, 314.43it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-08 18:34:45.204307] Computation for sample SRR1082520 finished. Elapsed time: 0:00:03.016723[2024-03-08 18:34:45.205115] Computation for sample SRR1074140 finished. Elapsed time: 0:00:03.023164\n",
      "[2024-03-08 18:34:45.205539] Computation for sample SRR1083776 finished. Elapsed time: 0:00:03.027317[2024-03-08 18:34:45.206525] Total extracted sites: 216. in SampleSRR811938\n",
      "\n",
      "\n",
      "[2024-03-08 18:34:45.207994] Total extracted sites: 196. in SampleSRR811938[2024-03-08 18:34:45.207742] Total extracted sites: 199. in SampleSRR811938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 940/940 [00:02<00:00, 314.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 940/940 [00:02<00:00, 316.81it/s]\n",
      "\n",
      "100%|█████████▉| 1015/1018 [00:03<00:00, 324.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-08 18:34:45.344834] Computation for sample SRR1071359 finished. Elapsed time: 0:00:03.159492\n",
      "[2024-03-08 18:34:45.346876] Total extracted sites: 189. in SampleSRR811938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1018/1018 [00:03<00:00, 327.89it/s]\n",
      " 98%|█████████▊| 1404/1429 [00:04<00:00, 317.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-08 18:34:46.639906] Computation for sample SRR811938 finished. Elapsed time: 0:00:04.455947\n",
      "[2024-03-08 18:34:46.641705] Total extracted sites: 310. in SampleSRR811938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1429/1429 [00:04<00:00, 324.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-08 18:34:46.718676] Computation Finished. Time elapsed 0:00:00.079709 minutes\n",
      "Cluster 0 Sites Extracted:1483\n"
     ]
    }
   ],
   "source": [
    "import os, gzip, sys, time, pysam\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from multiprocessing import Pool\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def extractor(sample_id, input_path_1, input_path_2, output_path, ohe, interval, suffix):\n",
    "    \n",
    "    intervals = []\n",
    "    columns = [\"Region\", \"Position\", \"Ref\", \"Strand\", \"Cov\", \"Qual\", \"[A,C,G,T]\", \"AllSubs\", \"Freq\", \"gCov\", \"gQual\", \"g[A,C,G,T]\", \"gAllSubs\", \"gFreq\", \"type\", \"sample\"]\n",
    "    if suffix == \"zeros\":\n",
    "        df_sites_all = pd.read_csv(os.path.join(input_path_2, f\"{sample_id}_{suffix}_filtered_candidates_list.tsv\"), sep=\"\\t\")\n",
    "    else:\n",
    "        df_sites_all = pd.read_csv(os.path.join(input_path_2, f\"{sample_id}_{suffix}_candidates_list.tsv\"), sep=\"\\t\") \n",
    "    df_sites_all.columns = [\"region\", \"position\", \"ref_base\", \"annotation\", \"label\"]\n",
    "    df = df_sites_all.query(\"region != 'chrM'\")\n",
    "    srr_filepath = os.path.join(input_path_1, sample, sample+'_dna.txt.gz')\n",
    "    print(f\"[{datetime.now()}] Loading reditable with tabix and pysam:\", srr_filepath)\n",
    "    start_time = datetime.now()\n",
    "    srr = pysam.TabixFile(srr_filepath)\n",
    "\n",
    "    features_extracted_filepath = \"{}/{}_{}_features_{}_nucleotides.tsv\".format(output_path, sample_id, suffix, interval)\n",
    "    features_extracted = open(features_extracted_filepath, \"w\")\n",
    "\n",
    "    with tqdm(total=df.shape[0], position=0, leave=True) as pbar:\n",
    "        for site in df.itertuples():\n",
    "            start = site.position - ((interval-1)/2)\n",
    "            stop = site.position + ((interval-1)/2)\n",
    "            srr_interval = []\n",
    "            for s in srr.fetch(site.region, start-1, stop):\n",
    "                srr_interval.append(s.split(\"\\t\"))\n",
    "            srr_interval = pd.DataFrame(srr_interval, columns=columns[:-2])\n",
    "            if srr_interval.shape[0] == interval:\n",
    "                counts = srr_interval.AllSubs.value_counts()\n",
    "                if \"AG\" in counts.index:\n",
    "                    if counts.loc[\"AG\"] >= 2:\n",
    "                        intervals.append([site.region, site.position, site.ref_base, site.annotation, site.label, sample, start, stop, stop-start + 1, srr_interval.shape[0]])\n",
    "                        seq = srr_interval.Ref.values.reshape(-1,1)\n",
    "                        seq_ohe = ohe.transform(seq).toarray().T\n",
    "                        vects_freqs = []\n",
    "                        vects = []\n",
    "                        for vect in srr_interval[\"[A,C,G,T]\"]:\n",
    "                            vect = np.array(eval(vect))\n",
    "                            cov = sum(vect)\n",
    "                            vect_freqs = vect / cov\n",
    "                            vects_freqs.append(vect_freqs)\n",
    "                            vects.append(vect)\n",
    "                        vects_freqs = np.array(vects_freqs).T\n",
    "                        vects = np.array(vects).T\n",
    "                        site = pd.concat([pd.DataFrame(seq_ohe), pd.DataFrame(vects_freqs)])\n",
    "                        site.to_csv(features_extracted, mode=\"a\", sep=\"\\t\", header = None, index=None)\n",
    "            pbar.update(1)\n",
    "   \n",
    "        intervals = pd.DataFrame(intervals, columns=[\"Region\", \"Position\", \"Ref_base\", \"Annotation\",\"Type\", \"SRR\", \"Start\", \"Stop\", \"DeltaStartStop\", \"TabixIntervalLen\"])\n",
    "        intervals.to_csv(os.path.join(output_path, f\"{sample_id}_{suffix}_feature_vectors_metadata.tsv\"), sep=\"\\t\", index=None)\n",
    "        lenght = len(intervals)\n",
    "        print(f\"[{datetime.now()}] Computation for sample {sample_id} finished. Elapsed time: {datetime.now()-start_time}\")\n",
    "        print(f\"[{datetime.now()}] Total extracted sites: {lenght}. in Sample{sample}\")\n",
    "\n",
    "        return lenght\n",
    "\n",
    "tissue, cluster_number, nucleotides_number, suffix = \"fallopian_tube\", 0, 101, \"snps\"\n",
    "\n",
    "tables_path = \"/lustre/biomed/epicardi/ncbi/dbGaP-6698/sra\"\n",
    "samples_path = \"/lustrehome/pietrolucamazzacuva/filezilla-recas/tissues/{}/{}_samples_lists\".format(tissue, tissue)\n",
    "features_path = \"/lustrehome/pietrolucamazzacuva/filezilla-recas/tissues/{}/{}_datasets\".format(tissue, tissue)\n",
    "if not os.path.isdir(features_path):\n",
    "    os.mkdir(features_path)\n",
    "\n",
    "samples = pd.read_csv(f\"/lustrehome/pietrolucamazzacuva/filezilla-recas/tissues/{tissue}/{tissue}_clusters/{tissue}_cluster_{cluster_number}.csv\", usecols=[\"Sample\"])\n",
    "samples = samples[\"Sample\"].tolist()\n",
    "onehot = OneHotEncoder()\n",
    "onehot.fit(np.array([\"A\", \"C\", \"G\", \"T\"]).reshape(-1, 1))\n",
    "\n",
    "total_sites = 0\n",
    "start_time_global = datetime.now()\n",
    "\n",
    "with Pool(15) as pool:\n",
    "    for sites in pool.starmap(extractor, [[sample, tables_path, samples_path, features_path, onehot, nucleotides_number, suffix] for sample in samples]):\n",
    "        total_sites += sites\n",
    "        \n",
    "stop_time_global = datetime.now()\n",
    "print(f\"[{datetime.now()}] Computation Finished. Time elapsed {(stop_time_global-start_time_global)/60} minutes\")\n",
    "print(f\"Cluster {cluster_number} Sites Extracted:{total_sites}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "21f0bca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.32it/s]\n",
      "100%|██████████| 24/24 [00:00<00:00, 4210.97it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.69it/s]\n",
      "100%|██████████| 1/1 [00:11<00:00, 11.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features matrices in dataset: 135904\n",
      "Time elapsed: 0.1972874124844869 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os, glob, time, random, sys\n",
    "import pandas as pd\n",
    "from multiprocessing import pool\n",
    "from tqdm import tqdm\n",
    "\n",
    "def counter(tissue_path, suffix):\n",
    "    counter = 0\n",
    "    for name in os.listdir(tissue_path):\n",
    "        if name.find(f\"{suffix}_feature_vectors_metadata.tsv\") != -1:\n",
    "            df = pd.read_csv(os.path.join(tissue_path, name), sep=\"\\t\", usecols=[\"Position\", \"Ref_base\"])\n",
    "            counter += df.shape[0]\n",
    "    return counter\n",
    "\n",
    "def converter(input_path, sample_id, input_indexes_1, input_indexes_2, input_indexes_3):\n",
    "    \n",
    "    cols = [i for i in range(101)]\n",
    "    cols.sort(reverse=True)\n",
    "    \n",
    "    features = pd.read_csv(os.path.join(input_path, f\"{sample_id}_positives_features_101_nucleotides.tsv\"), \n",
    "                           sep=\"\\t\", header=None)\n",
    "    features_a = features.iloc[input_indexes_1, :]\n",
    "    features_a.reset_index(drop=True, inplace=True)\n",
    "    features_t = features.iloc[input_indexes_2, :]\n",
    "    features_t.reset_index(drop=True, inplace=True)\n",
    "    del features\n",
    "    features_t = features_t.iloc[:, cols]\n",
    "    old_index = pd.Index(data=[f\"{i}\" for i in range(features_t.shape[0])])\n",
    "    features_t.set_index(old_index, drop=True, inplace=True)\n",
    "    new_index = []\n",
    "    for i in range(0, features_t.shape[0]-7, 8):\n",
    "        new_index += [f\"{i+3}\", f\"{i+2}\", f\"{i+1}\", f\"{i}\", f\"{i+7}\", f\"{i+6}\", f\"{i+5}\", f\"{i+4}\"] \n",
    "    features_t = features_t.reindex(new_index)\n",
    "        \n",
    "    features_snps = pd.read_csv(os.path.join(input_path, f\"{sample_id}_snps_features_101_nucleotides.tsv\"), sep=\"\\t\", header=None)\n",
    "        \n",
    "    if len(input_indexes_3) > 0:\n",
    "        features_zero = pd.read_csv(os.path.join(input_path, f\"{sample_id}_zeros_features_101_nucleotides.tsv\"), \n",
    "                                    sep=\"\\t\", header=None)\n",
    "        features_zero = features_zero.iloc[input_indexes_3, :]\n",
    "        features_zero.reset_index(drop=True, inplace=True)\n",
    "    else:\n",
    "        features_zero = pd.DataFrame()\n",
    "        \n",
    "    cols = [f\"{i}\" for i in range(-50, 51, 1)]\n",
    "    \n",
    "    data_list = []\n",
    "    if features_a.shape[0] > 0:\n",
    "        features_a.columns = cols\n",
    "        data_list.append(features_a)\n",
    "    if features_t.shape[0] > 0:\n",
    "        features_t.columns = cols\n",
    "        data_list.append(features_t)\n",
    "    if features_snps.shape[0] > 0:\n",
    "        features_snps.columns = cols\n",
    "        data_list.append(features_snps)\n",
    "    if features_zero.shape[0] > 0:\n",
    "        features_zero.columns = cols\n",
    "        data_list.append(features_zero)\n",
    "        \n",
    "    data_list = pd.concat(data_list, axis=0)\n",
    "    data_list.to_csv(os.path.join(input_path, f\"{sample_id}_features_reduced_pos_zeros_snps.tsv\"), sep=\"\\t\", header=False, index=None) \n",
    "    \n",
    "    return data_list.shape[0]\n",
    "\n",
    "zeros_tissue = \"fallopian_tube\"\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "tissues = [\"fallopian_tube\"]\n",
    "\n",
    "chrs = [\"chr1\", \"chr2\", \"chr3\", \"chr4\", \"chr5\", \"chr6\", \n",
    "        \"chr7\", \"chr8\", \"chr9\", \"chr10\", \"chr11\", \"chr12\",\n",
    "        \"chr13\", \"chr14\", \"chr15\", \"chr16\", \"chr17\", \"chr18\",\n",
    "        \"chr19\", \"chr20\", \"chr21\", \"chr22\", \"chrX\", \"chrY\"]\n",
    "\n",
    "columns = [\"Region\", \"Position\", \"Ref_base\", \"Annotation\", \"Type\", \"SRR\", \"Start\", \"Stop\", \"DeltaStartStop\", \"TabixIntervalLen\"]\n",
    "\n",
    "path = \"/lustrehome/pietrolucamazzacuva/filezilla-recas/tissues\"\n",
    "\n",
    "positives = pd.DataFrame()\n",
    "for tissue in tqdm(tissues):\n",
    "    data_path = os.path.join(path, f\"{tissue}/{tissue}_datasets\")\n",
    "    for filename in glob.iglob(f\"{data_path}/*_positives_feature_vectors_metadata.tsv\"):\n",
    "        sample = filename.replace(\"_positives_feature_vectors_metadata.tsv\", \"\")\n",
    "        sample = sample.replace(data_path+\"/\", \"\")\n",
    "        df = pd.read_csv(filename, sep=\"\\t\", usecols=[\"Region\", \"Position\", \"Annotation\"])\n",
    "        df[\"Sample\"] = sample\n",
    "        df[\"Position_In_Dataset\"] = [i for i in range(df.shape[0])]\n",
    "        positives=pd.concat([positives, df], axis=0)\n",
    "\n",
    "del data_path, sample, df\n",
    "\n",
    "positives.reset_index(drop=True, inplace=True)    \n",
    "index = positives[positives.loc[:, \"Annotation\"] != \"NOT_ALU_POSITIVE\"].index.tolist()\n",
    "positives.drop(\"Annotation\", axis=1, inplace=True)\n",
    "positives_rep_not_alu = positives.iloc[index, :]\n",
    "positives_rep_not_alu.drop([\"Region\", \"Position\"], axis=1, inplace=True)\n",
    "positives.drop(index, axis=0, inplace=True)\n",
    "positives.reset_index(drop=True, inplace=True)    \n",
    "        \n",
    "region_position = positives.loc[:, [\"Region\", \"Position\"]]\n",
    "sample_position = positives.loc[:, [\"Sample\", \"Position_In_Dataset\"]]\n",
    "\n",
    "del positives\n",
    "\n",
    "value_counts = region_position.value_counts()\n",
    "upper_5 = value_counts[value_counts.loc[:]>5].index.tolist()\n",
    "\n",
    "del value_counts\n",
    "\n",
    "drops = []\n",
    "\n",
    "with tqdm(total = 24) as pbar:\n",
    "    for Chr in chrs:\n",
    "        globals()[Chr] = region_position[region_position[\"Region\"]==Chr]\n",
    "        pbar.update(1)\n",
    "\n",
    "del region_position\n",
    "\n",
    "with tqdm(total = len(upper_5)) as pbar:\n",
    "    start = time.time()\n",
    "    for region, position in upper_5:\n",
    "        temp = globals()[region][globals()[region][\"Position\"]==position].index.tolist()\n",
    "        del globals()[region]\n",
    "        temp = random.sample(temp, len(temp)-5)\n",
    "        drops += temp\n",
    "        pbar.update(1)\n",
    "\n",
    "drops.sort()\n",
    "\n",
    "sample_position.drop(drops, axis=0, inplace=True)\n",
    "sample_position = pd.concat([sample_position, positives_rep_not_alu], axis=0)\n",
    "sample_position.sort_values([\"Sample\", \"Position_In_Dataset\"], inplace=True)\n",
    "sample_position.to_csv(os.path.join(path, \"positives_filtered_list.tsv\"), sep=\"\\t\", index=None)\n",
    "\n",
    "n_positives = sample_position.shape[0]\n",
    "del sample_position, drops\n",
    "\n",
    "inputs =[]\n",
    "for tissue in tissues:\n",
    "    inputs.append([os.path.join(path, f\"{tissue}/{tissue}_datasets\"), \"snps\"])\n",
    "\n",
    "n_snps = 0   \n",
    "with tqdm(total = len(inputs)) as pbar:\n",
    "    with Pool(32) as pool:\n",
    "        for result in pool.starmap(counter, inputs):\n",
    "            n_snps += result\n",
    "            pbar.update(1)\n",
    "\n",
    "n_zeros = counter(os.path.join(path, f\"{zeros_tissue}/{zeros_tissue}_datasets\"), \"zeros\")\n",
    "\n",
    "filtered_positions = pd.read_csv(os.path.join(path, \"positives_filtered_list.tsv\"), sep=\"\\t\")\n",
    "zero_frac = round((n_positives-n_snps)/n_zeros, 9)\n",
    "total_features = 0\n",
    "with tqdm(total = len(tissues)) as pbar:\n",
    "    for tissue in tissues:\n",
    "\n",
    "        data_path = \"{}/{}/{}_datasets\".format(path, tissue, tissue)\n",
    "        clusters_path = \"{}/{}/{}_clusters\".format(path, tissue, tissue)\n",
    "        \n",
    "        n_clusters = len(os.listdir(clusters_path))\n",
    "        for cluster_number in range(n_clusters):\n",
    "            \n",
    "            samples = pd.read_csv(os.path.join(clusters_path, f\"{tissue}_cluster_{cluster_number}.csv\"), usecols=[\"Sample\"])\n",
    "            samples = samples[\"Sample\"].tolist()\n",
    "\n",
    "            inputs = []\n",
    "            for sample in samples:\n",
    "                metadata = pd.DataFrame()\n",
    "                indexes_list = filtered_positions[filtered_positions[\"Sample\"]==sample].loc[:, \"Position_In_Dataset\"].tolist()\n",
    "                meta = pd.read_csv(os.path.join(data_path, f\"{sample}_positives_feature_vectors_metadata.tsv\"), \n",
    "                                   sep=\"\\t\", usecols=columns)\n",
    "                meta = meta.iloc[indexes_list, :]\n",
    "                index_a = meta[meta[\"Ref_base\"] ==  \"A\"].index.tolist()\n",
    "                index_t = meta[meta[\"Ref_base\"] ==  \"T\"].index.tolist()\n",
    "                indexes_a = []\n",
    "                indexes_t = []\n",
    "                for index in index_a:\n",
    "                    indexes_a += [int(index)*8+i for i in range(8)]\n",
    "                for index in index_t:\n",
    "                    indexes_t += [int(index)*8+i for i in range(8)] \n",
    "                meta_a = meta.loc[index_a, :] \n",
    "                meta_t = meta.loc[index_t, :] \n",
    "                metadata = pd.concat([metadata, meta_a, meta_t], axis=0)\n",
    "                del meta\n",
    "                                  \n",
    "                meta =  pd.read_csv(os.path.join(data_path, f\"{sample}_snps_feature_vectors_metadata.tsv\"), \n",
    "                                   sep=\"\\t\", usecols=columns)   \n",
    "                \n",
    "                metadata = pd.concat([metadata, meta], axis=0)\n",
    "                del meta\n",
    "                \n",
    "                indexes_zero = []\n",
    "                if os.path.isfile(os.path.join(data_path, f\"{sample}_zeros_feature_vectors_metadata.tsv\")):\n",
    "                    meta = pd.read_csv(os.path.join(data_path, f\"{sample}_zeros_feature_vectors_metadata.tsv\"), \n",
    "                                       sep=\"\\t\", usecols=columns)\n",
    "                    meta = meta.sample(frac=zero_frac, random_state=42)\n",
    "                    metadata = pd.concat([metadata, meta], axis=0)\n",
    "                    zero_index = meta.index.tolist()\n",
    "                    indexes_zero= []\n",
    "                    for index in zero_index:\n",
    "                        indexes_zero += [int(index)*8+i for i in range(8)]\n",
    "                    del meta\n",
    "                    \n",
    "                inputs.append([data_path, sample, indexes_a, indexes_t, indexes_zero])\n",
    "                \n",
    "                \n",
    "                metadata.to_csv(os.path.join(data_path, f\"{sample}_metadata_reduced_pos_zeros_snps.tsv\"), sep=\"\\t\", index=None)\n",
    "                del metadata\n",
    "                \n",
    "            with Pool(32) as pool:\n",
    "                for result in pool.starmap(converter, inputs):\n",
    "                    total_features += int(result/8)\n",
    "\n",
    "        pbar.update(1) \n",
    "\n",
    "print(f\"Total features matrices in dataset: {total_features}\")\n",
    "end = time.time()\n",
    "print(f\"Time elapsed: {(end-start)/60} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08036a09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
